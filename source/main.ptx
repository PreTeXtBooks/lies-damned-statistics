<?xml version="1.0" encoding="UTF-8"?>
<pretext xml:lang="en-US">
  <book xml:id="ldlos">
    <title>Lies, Damned Lies, or Statistics</title>
    <subtitle>How to Tell the Truth with Statistics</subtitle>
    <author>
      <personname>
        <firstname>Jonathan</firstname>
        <middlename>A.</middlename>
        <surname>Poritz</surname>
      </personname>
    </author>

    <frontmatter>
      <preface xml:id="release-notes">
        <title>Release Notes</title>
        <p/>
      </preface>
      <preface xml:id="preface">
        <title>Preface</title>
        <p/>
      </preface>
    </frontmatter>

    <part xml:id="part-DS">
      <title>Descriptive Statistics</title>

      <chapter xml:id="chap-1VS">
        <title>One-Variable Statistics: Basics</title>
        <section xml:id="sec-TIPVS">
          <title>Terminology: Individuals/Population/Variables/Samples</title>
          <p>
            Oddly enough, it is often a lack of clarity about <em>who</em> [or <em>what</em>]
            <em>you are looking at</em> which makes a lie out of statistics. Here are the
            terms, then, to keep straight:
          </p>

          <definition xml:id="def-individual-population">
            <idx>individual in a statistical study</idx>
            <idx>population of a statistical study</idx>
            <statement>
              <p>
                The units which are the objects of a statistical study are called the
                <term>individuals</term> in that study,
                while the collection of all such individuals is called the
                <term>population</term> of the study.
              </p>
            </statement>
          </definition>

          <p>
            Note that while the term <q>individuals</q> sounds like it is talking about
            people, the individuals in a study could be things, even abstract things like
            events.
          </p>

          <example xml:id="eg-votersind">
            <title>Individuals in Election Study</title>
            <idx>voters</idx>
            <statement>
              <p>
                The individuals in a study about a democratic election might be
                <em>the voters</em>. But if you are going to make an accurate
                prediction of who will win the election, it is important to be more precise
                about what exactly is the population of all of those individuals [voters]
                that you intend to study: is it <em>all eligible voters</em>, <em>all
                registered voters</em>, <em>the people who actually voted</em>, <em>etc.</em>
              </p>
            </statement>
          </example>

          <example xml:id="eg-flipsind">
            <title>Individuals in Coin Flip Study</title>
            <statement>
              <p>
                If you want to study if a coin is <q>fair</q> or not, you would flip it repeatedly.
                The individuals would then be <em>flips of that coin</em>, and the population
                might be something like <em>all the flips ever done in the past and all that
                will ever be done in the future</em>. These individuals are quite abstract, and
                in fact it is impossible ever to get your hands on all of them (the ones in the
                future, for example).
              </p>
            </statement>
          </example>

          <example xml:id="eg-studentstakingtestsind">
            <title>Individuals in Homework Study</title>
            <statement>
              <p>
                Suppose we're interested in studying whether doing more homework helps students
                do better in their studies. So shouldn't the individuals be the students?
                Well, which students? How about we look only at college students. Which
                college students? OK, how about students at 4-year colleges and universities
                in the United States, over the last five years<mdash/>after all, things might be
                different in other countries and other historical periods.
              </p>
              <p>
                Wait, a particular student might sometimes do a lot of homework and sometimes
                do very little. And what exactly does <q>do better in their studies</q> mean? So
                maybe we should look at each student in each class they take, then we can look
                at the homework they did for that class and the success they had in it.
              </p>
              <p>
                Therefore, the individuals in this study would be <em>individual experiences
                that students in US 4-year colleges and universities had in the last five
                years</em>, and population of the study would essentially be the collection of all
                the names on all class rosters of courses in the last five years at all US
                4-year colleges and universities.
              </p>
            </statement>
          </example>

          <p>
            When doing an actual scientific study, we are usually not interested so much
            in the individuals themselves, but rather in
          </p>

          <definition xml:id="def-variable">
            <idx>variable</idx>
            <idx><h>variable</h><h>categorical</h></idx>
            <idx><h>variable</h><h>quantitative</h></idx>
            <statement>
              <p>
                A <term>variable</term> in a statistical study is the answer of a question the
                researcher is asking about each individual. There are two types:
                <ul>
                  <li>
                    <p>
                      A <term>categorical variable</term> is one whose
                      values have a finite number of possibilities.
                    </p>
                  </li>
                  <li>
                    <p>
                      A <term>quantitative variable</term> is one whose
                      values are numbers (so, potentially an infinite number of possibilities).
                    </p>
                  </li>
                </ul>
              </p>
            </statement>
          </definition>

          <p>
            The variable is something which (as the name says) <em>varies</em>, in the sense
            that it can have a different value for each individual in the population
            (although that is not necessary).
          </p>

          <example xml:id="eg-votersvar">
            <title>Variable in Election Study</title>
            <statement>
              <p>
                In <xref ref="eg-votersind"/>, the variable most likely would be <em>who they
                voted for</em>, a categorical variable with only possible values <q>Mickey Mouse</q>
                or <q>Daffy Duck</q> (or whoever the names on the ballot were).
              </p>
            </statement>
          </example>

          <example xml:id="eg-flipsvar">
            <title>Variable in Coin Flip Study</title>
            <statement>
              <p>
                In <xref ref="eg-flipsind"/>, the variable most likely would be <em>what face
                of the coin was facing up after the flip</em>, a categorical variable with values
                <q>heads</q> and <q>tails.</q>
              </p>
            </statement>
          </example>

          <example xml:id="eg-studentstakingtestsvar">
            <title>Variables in Homework Study</title>
            <statement>
              <p>
                There are several variables we might use in
                <xref ref="eg-studentstakingtestsind"/>. One might be <em>how many homework
                problems did the student do in that course</em>. Another could be <em>how many
                hours total did the student spend doing homework over that whole semester, for
                that course</em>. Both of those would be quantitative
                variables.
              </p>
              <p>
                A categorical variable for the same
                population would be <em>what letter grade did the student get in the course</em>,
                which has possible values <term>A</term>, <term>A-</term>, <term>B+</term>, <ellipsis/>, <term>D-</term>, <term>F</term>.
              </p>
            </statement>
          </example>

          <p>
            In many [most?] interesting studies, the population is too large for it to be
            practical to go observe the values of some interesting variable. Sometimes it
            is not just impractical, but actually impossible<mdash/>think of the example we gave
            of all the flips of the coin, even the ones in the future. So instead, we
            often work with
          </p>

          <definition xml:id="def-sample">
            <idx>sample</idx>
            <statement>
              <p>
                A <term>sample</term> is a subset of a population under study.
              </p>
            </statement>
          </definition>

          <p>
            Often we use the variable
            <m>N</m> to indicate the
            size of a whole population and the variable
            <m>n</m> for the size of a sample;
            as we have said, usually <m>n \lt N</m>.
          </p>

          <p>
            Later we shall discuss how to pick a good sample, and how much we can learn
            about a population from looking at the values of a variable of interest only
            for the individuals in a sample. For the rest of this chapter, however, let's
            just consider what to do with these sample values.
          </p>
        </section>
        <section xml:id="sec-VRoDICV">
          <title>Visual Representation of Data, I: Categorical Variables</title>
          <p>
            Suppose we have a population and variable in which we are interested. We get
            a sample, which could be large or small, and look at the values of our
            variable for the individuals in that sample. We shall informally refer to
            this collection of values as a <term>dataset</term>.
          </p>

          <p>
            In this section, we suppose also that the variable we are looking at is
            categorical. Then we can summarize the dataset by telling which categorical
            values did we see for the individuals in the sample, and how often we saw
            those values.
          </p>

          <p>
            There are two ways we can make pictures of this information: <em>bar charts</em>
            and <em>pie charts</em>.
          </p>
          <subsection xml:id="ssec-BCIFC">
            <title>Bar Charts I: Frequency Charts</title>
            <p>
              We can take the values which we saw for individuals in the sample along the
              <m>x</m>-axis of a graph, and over each such label make a box whose height indicates
              how many individuals had that value<mdash/>the <term>frequency</term> of occurrence of
              that value.
            </p>

            <p>
              This is called a <term>bar chart</term>. As with all graphs, you
              should <em>always label all axes.</em> The <m>x</m>-axis will be labeled with some
              description of the variable in question, the <m>y</m>-axis label will always be
              <q>frequency</q> (or some synonym like <q>count</q> or <q>number of times</q>).
            </p>

            <example xml:id="eg-flipsbarchartfreq">
              <title>Frequency Bar Chart for Coin Flips</title>
              <statement>
                <p>
                  In <xref ref="eg-flipsvar"/>, suppose we took a sample consisting of the
                  next 10 flips of our coin. Suppose further that 4 of the flips came up
                  heads<mdash/>write it as <q>H</q><mdash/>and 6 came up tails, T. Then the corresponding
                  bar chart would show bars for H with height 4 and T with height 6.
                </p>
                <p>
                  <em>[Figure showing frequency bar chart with H at 4 and T at 6 would go here]</em>
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-BCIIRFC">
            <title>Bar Charts II: Relative Frequency Charts</title>
            <p>
              There is a variant of the above kind of bar chart which actually looks nearly
              the same but changes the labels on the <m>y</m>-axis. That is, instead of making
              the height of each bar be how many times each categorical value occurred, we
              could make it be <em>what fraction of the sample had that categorical
              value</em><mdash/>the <term>relative frequency</term>. This fraction is often displayed as a
              percentage.
            </p>

            <example xml:id="eg-flipsbarchartrelfreq">
              <title>Relative Frequency Bar Chart for Coin Flips</title>
              <statement>
                <p>
                  The relative frequency version of the above bar chart in
                  <xref ref="eg-flipsbarchartfreq"/> would show bars for H at height 0.4 (or 40%)
                  and T at height 0.6 (or 60%).
                </p>
                <p>
                  <em>[Figure showing relative frequency bar chart would go here]</em>
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-BCIIIC">
            <title>Bar Charts III: Cautions</title>
            <p>
              Notice that with bar charts (of either frequency or relative frequency) the
              variable values along the <m>x</m>-axis <em>can appear in any order whatsoever</em>.
              This means that any conclusion you draw from looking at the bar chart must
              not depend upon that order. For example, it would be foolish to say that the
              graph in <xref ref="eg-flipsbarchartfreq"/> <q>shows an increasing
              trend,</q> since it would make just as much sense to put the bars in the other
              order and then <q>show a decreasing trend</q><mdash/>both are meaningless.
            </p>

            <p>
              For relative frequency bar charts, in particular, note that the total of
              the heights of all the bars must be <m>1</m> (or <m>100\%</m>). If it is more, something
              is weird; if it is less, some data has been lost.
            </p>

            <p>
              Finally, it makes sense for both kinds of bar charts for the <m>y</m>-axis to
              run from the logical minimum to maximum. For frequency charts, this means it
              should go from <m>0</m> to <m>n</m> (the sample size). For relative frequency charts,
              it should go from <m>0</m> to <m>1</m> (or <m>100\%</m>). Skimping on how much of this
              appropriate <m>y</m>-axis is used is a common trick to lie with statistics.
            </p>

            <example xml:id="eg-flipsbarchartrelfreqbadyaxis">
              <title>Misleading Bar Chart with Bad Y-Axis</title>
              <statement>
                <p>
                  The coin we looked at in <xref ref="eg-flipsbarchartfreq"/> and
                  <xref ref="eg-flipsbarchartrelfreq"/> could well be a fair coin<mdash/>it didn't
                  show exactly half heads and half tails, but it was pretty close. Someone who
                  was trying to claim, deceptively, that the coin was not fair might have shown
                  only a portion of the <m>y</m>-axis, making the difference appear much more dramatic.
                </p>
                <p>
                  <em>[Figure showing bar chart with restricted y-axis from 0.3 to 0.6 would go here]</em>
                </p>
                <p>
                  This is actually, in a strictly technical sense, a correct graph. But, looking
                  at it, one might conclude that T seems to occur more than twice as often as
                  H, so the coin is probably not fair<ellipsis/> until a careful examination of the
                  <m>y</m>-axis shows that even though the bar for T is more than twice as high as the
                  bar for H, that is an artifact of how much of the <m>y</m>-axis is being shown.
                </p>
              </statement>
            </example>

            <p>
              In summary, bar charts actually don't have all that much use in sophisticated
              statistics, but are extremely common in the popular press (and on web sites
              and so on).
            </p>
          </subsection>
          <subsection xml:id="ssec-PC">
            <title>Pie Charts</title>
            <p>
              Another way to make a picture with categorical data is to use the fractions
              from a relative frequency bar chart, but not for the heights of bars, instead
              for the sizes of wedges of a pie.
            </p>

            <example xml:id="eg-flipspiechart">
              <title>Pie Chart for Coin Flips</title>
              <statement>
                <p>
                  Here's a pie chart with the relative frequency data from
                  <xref ref="eg-flipsbarchartrelfreq"/>. It would show a pie divided into two
                  wedges: one for H taking up 40% of the pie, and one for T taking up 60%.
                </p>
                <p>
                  <em>[Figure showing pie chart would go here]</em>
                </p>
              </statement>
            </example>

            <p>
              Pie charts are widely used, but actually they are almost never a good choice.
              In fact, do an Internet search for the phrase <q>pie charts are bad</q> and there
              will be many hits. Many of the arguments are quite insightful.
            </p>

            <p>
              When you see a pie chart, it is either an attempt (misguided, though) by
              someone to be folksy and friendly, or it is a sign that the author is quite
              unsophisticated with data visualization, or, worst of all, it might be a sign
              that the author is trying to use mathematical methods in a deceptive way.
            </p>

            <p>
              In addition, all of the cautions we mentioned above for bar charts of
              categorical data apply, mostly in exactly the same way, for pie charts.
            </p>
          </subsection>
        </section>
        <section xml:id="sec-VRoDIIQV">
          <title>Visual Representation of Data, II: Quantitative Variables</title>
          <p>
            Now suppose we have a population and <em>quantitative</em>
            variable in which we
            are interested. We get a sample, which could be large or small, and look at
            the values of our variable for the individuals in that sample. There are
            two ways we tend to make pictures of datasets like this: <em>stem-and-leaf
            plots</em> and <em>histograms</em>.
          </p>
          <subsection xml:id="ssec-SalP">
            <title>Stem-and-leaf Plots</title>
            <idx>stem, in stemplot</idx>
            <idx>leaf, in stemplot</idx>
            <idx>stem-and-leaf plot, stemplot</idx>
            <p>
              One somewhat old-fashioned way to handle a modest amount of quantitative data
              produces something between simply a list of all the data values and a graph.
              It's not a bad technique to know about in case one has to write down a dataset
              by hand, but very tedious <mdash/> and quite unnecessary, if one uses modern
              electronic tools instead <mdash/> if the dataset has more than a couple dozen values.
              The easiest case of this technique is where the data are all whole numbers in
              the range <m>0-99</m>. In that case, one can take off the tens place of each
              number <mdash/> call it the <term>stem</term> <mdash/> and put it on the
              left side of a vertical bar, and then line up all the ones places <mdash/> each is a
              <term>leaf</term> <mdash/> to the right of that stem. The whole
              thing is called a <term>stem-and-leaf plot</term>
              or, sometimes, just a <term>stemplot</term>.
            </p>

            <p>
              It's important not to skip any stems which are in the middle of the dataset,
              even if there are no corresponding leaves. It is also a good idea to allow
              repeated leaves, if there are repeated numbers in the dataset, so that the
              length of the row of leaves will give a good representation of how much
              data is in that general group of data values.
            </p>

            <example xml:id="eg-stemandleafplot">
              <idx>stem-and-leaf plot, stemplot</idx>
              <statement>
                <p>
                  Here is a list of the scores of 30 students on a statistics test:
                </p>
                <me>
                  \begin{matrix}
                  86 &amp; 80 &amp; 25 &amp; 77 &amp; 73 &amp; 76 &amp; 88 &amp; 90 &amp; 69 &amp; 93\\
                  90 &amp; 83 &amp; 70 &amp; 73 &amp; 73 &amp; 70 &amp; 90 &amp; 83 &amp; 71 &amp; 95\\
                  40 &amp; 58 &amp; 68 &amp; 69 &amp; 100 &amp; 78 &amp; 87 &amp; 25 &amp; 92 &amp; 74
                  \end{matrix}
                </me>
                <p>
                  As we said, using the tens place (and the hundreds place as well, for the
                  data value <m>100</m>) as the stem and the ones place as the leaf, we get
                </p>
                <p>
                  <em>[Table showing stem-and-leaf plot of students' scores, Key: <m>1 | 7 = 17</m> would go here]</em>
                </p>
              </statement>
            </example>

            <p>
              One nice feature stem-and-leaf plots have is that <em>they contain
              all of the data values</em>, they do not lose anything (unlike our next
              visualization method, for example).
            </p>
          </subsection>
          <subsection xml:id="ssec-FHistograms">
            <title>[Frequency] Histograms</title>
            <idx>histogram</idx>
            <idx>bins, in a histogram</idx>
            <idx>classes, in a histogram</idx>
            <p>
              The most important visual representation of quantitative data is a
              <term>histogram</term>. Histograms actually look a lot like a stem-and-leaf plot,
              except turned on its side and with the row of numbers turned into a vertical
              bar, like a bar graph. The height of each of these bars would be how many
            </p>

            <p>
              Another way of saying that is that we would be making bars whose heights were
              determined by how many scores were in each group of ten. Note there is still
              a question of into which bar a value right on the edge would count: <em>e.g.,</em>
              does the data value <m>50</m> count in the bar to the left of that number, or the
              bar to the right? It doesn't actually matter which side, but it is important
              to state which choice is being made.
            </p>

            <example xml:id="eg-scoreshistbytens">
              <idx>histogram</idx>
              <statement>
                <p>
                  Continuing with the score data in Example <xref ref="eg-stemandleafplot"/> and
                  putting all data values <m>x</m> satisfying <m>20\le x&lt;30</m> in the first bar,
                  values <m>x</m> satisfying <m>30\le x&lt;40</m> in the second,
                  values <m>x</m> satisfying <m>40\le x&lt;50</m> in the third, <em>etc.</em> <mdash/> that is,
                  put data values on the edges in the bar to the right <mdash/> we get the
                  figure
                </p>
                <p>
                  <em>[Figure showing scores histogram with binwidth 10 would go here]</em>
                </p>
              </statement>
            </example>

            <p>
              Actually, there is no reason that the bars always have to be ten units wide:
              it is important that they are all the same size and that how they handle the
              edge cases (whether the left or right bar gets a data value on edge), but
              they could be any size. We call the successive ranges of the <m>x</m> coordinates
              which get put together for each bar
              <term>bins</term> or
              <term>classes</term>, and it is up to the statistician
              to chose whichever bins <mdash/> where they start and how wide they are <mdash/> shows the
              data best.
            </p>

            <p>
              Typically, the smaller the bin size, the more variation (precision) can be
              seen in the bars <ellipsis/> but sometimes there is so much variation that the result
              seems to have a lot of random jumps up and down, like static on the radio.
              On the other hand, using a large bin size makes the picture smoother <ellipsis/> but
              sometimes, it is so smooth that very little information is left. Some of this
              is shown in the following
            </p>

            <example xml:id="eg-scoreshistvariousbins">
              <idx>histogram</idx>
              <statement>
                <p>
                  Continuing with the score data in Example <xref ref="eg-stemandleafplot"/> and now
                  using the bins
                  with <m>x</m> satisfying <m>10\le x&lt;12</m>, then <m>12\le x&lt;14</m>, <em>etc.</em>,
                  we get the histogram with bins of width 2:
                </p>
                <p>
                  <em>[Figure showing scores histogram with binwidth 2 would go here]</em>
                </p>

                <p>
                  If we use the bins with <m>x</m> satisfying <m>10\le x&lt;15</m>, then <m>15\le x&lt;20</m>,
                  <em>etc.</em>, we get the histogram with bins of width 5:
                </p>
                <p>
                  <em>[Figure showing scores histogram with binwidth 5 would go here]</em>
                </p>

                <p>
                  If we use the bins with <m>x</m> satisfying <m>20\le x&lt;40</m>, then <m>40\le x&lt;60</m>,
                  <em>etc.</em>, we get the histogram with bins of width 20:
                </p>
                <p>
                  <em>[Figure showing scores histogram with binwidth 20 would go here]</em>
                </p>

                <p>
                  Finally, if we use the bins with <m>x</m> satisfying <m>0\le x&lt;50</m>, then
                  <m>50\le x&lt;100</m>, and then <m>100\le x&lt;150</m>, we get the histogram with bins of
                  width 50:
                </p>
                <p>
                  <em>[Figure showing scores histogram with binwidth 50 would go here]</em>
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-RFHistograms">
            <title>[Relative Frequency] Histograms</title>
            <idx><h>histogram</h><h>relative frequency</h></idx>
            <p>
              Just as we could have bar charts with absolute (<xref ref="ssec-BCIFC"/>) or
              relative (<xref ref="ssec-BCIIRFC"/>) frequencies, we can do the same for
              histograms. Above, in
              <xref ref="ssec-FHistograms"/>, we made absolute frequency histograms. If, instead,
              we divide each of the counts used to determine the heights of the bars by the
              total sample size, we will get fractions or percents <mdash/> <em>relative</em>
              frequencies. We should then change the label on the <m>y</m>-axis and the
              tick-marks numbers on the <m>y</m>-axis, but otherwise the graph will look exactly
              the same (as it did with relative frequency bar charts compared with absolute
              frequency bar chars).
            </p>

            <example xml:id="eg-scoresRFhistbytens">
              <idx><h>histogram</h><h>relative frequency</h></idx>
              <statement>
                <p>
                  Let's make the relative frequency histogram corresponding to the absolute
                  frequency histogram in Example <xref ref="eg-scoreshistbytens"/>, based on the data
                  from Example <xref ref="eg-stemandleafplot"/> <mdash/> all we have to do is change the
                  numbers used to make heights of the bars in the graph by dividing them by
                  the sample size, 30, and then also change the <m>y</m>-axis label and tick mark
                  numbers.
                </p>
                <p>
                  <em>[Figure showing scores relative frequency histogram with binwidth 10 would go here]</em>
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-HtTAH">
            <title>How to Talk About Histograms</title>
            <idx>distribution</idx>
            <idx><h>shape</h><h>histogram</h></idx>
            <idx>center of a histogram, dataset, or distribution</idx>
            <idx>spread of a histogram, dataset, or distribution</idx>
            <idx>symmetric histogram, dataset, or distribution</idx>
            <idx>skewed histogram, dataset, or distribution</idx>
            <idx>unimodal histogram, dataset, or distribution</idx>
            <idx>multimodal histogram, dataset, or distribution</idx>
            <p>
              Histograms of course tell us what the data values are <mdash/> the location along
              the <m>x</m> value of a bar is the value of the variable <mdash/> and how many of them
              have each particular value <mdash/> the height of the bar tells how many data values
              are in that bin. This is also given a technical name
            </p>

            <definition xml:id="def-distribution">
              <idx>distribution</idx>
              <statement>
                <p>
                  Given a variable defined on a population, or at least on a sample, the
                  <term>distribution</term> of that variable is a list of all the values the variable
                  actually takes on and how many times it takes on these values.
                </p>
              </statement>
            </definition>

            <p>
              The reason we like the visual version of a distribution, its histogram, is
              that our visual intuition can then help us answer general, qualitative
              questions about what those data must be telling us. The first questions we
              usually want to answer quickly about the data are
            </p>
            <ul>
              <li>
                <p>
                  What is the <em>shape</em> of the histogram?
                </p>
              </li>
              <li>
                <p>
                  Where is its <em>center</em>?
                </p>
              </li>
              <li>
                <p>
                  How much <em>variability</em> [also called
                  <em>spread</em>] does it show?
                </p>
              </li>
            </ul>

            <p>
              When we talk about the general shape of a histogram, we often use the terms
            </p>

            <definition xml:id="def-symmskew">
              <idx>symmetric histogram, dataset, or distribution</idx>
              <idx>skewed histogram, dataset, or distribution</idx>
              <idx>unimodal histogram, dataset, or distribution</idx>
              <idx>multimodal histogram, dataset, or distribution</idx>
              <statement>
                <p>
                  A histogram is <term>symmetric</term> if the left half is (approximately) the mirror
                  image of the right half.
                </p>

                <p>
                  We say a histogram is <term>skewed left</term> if the tail on the left side is longer
                  than on the right. In other words, left skew is when the left half of the
                  histogram <mdash/> half in the sense that the total of the bars in this left part
                  is half of the size of the dataset <mdash/> extends farther to the left than the
                  right does to the right. Conversely, the histogram is <term>skewed right</term> if
                  the right half extends farther to the right than the left does to the left.
                </p>

                <p>
                  If the shape of the histogram has one significant peak, then we say it is
                  <term>unimodal</term>, while if it has several such, we say it is <term>multimodal</term>.
                </p>
              </statement>
            </definition>

            <p>
              It is often easy to point to where the center of a distribution <em>looks
              like</em> it lies, but it is hard to be precise. It is particularly difficult
              if the histogram is <q>noisy,</q> maybe multimodal. Similarly, looking at a
              histogram, it is often easy to say it is <q>quite spread out</q> or <q>very
              concentrated in the center,</q> but it is then hard to go beyond this general
              sense.
            </p>

            <p>
              Precision in our discussion of the center and spread of a dataset will
              only be possible in the next section, when we work with numerical measures
              of these features.
            </p>
          </subsection>
        </section>
        <section xml:id="sec-NDoDIMotC">
          <title>Numerical Descriptions of Data, I: Measures of the Center</title>
          <p>
            Oddly enough, there are several measures of central tendency, as ways to
            define the middle of a dataset are called.  There is different work to be
            done to calculate each of them, and they have different uses, strengths, and
            weaknesses.
          </p>

          <p>
            For this whole section we will assume we have collected <m>n</m> numerical values,
            the values of our quantitative
            variable<idx>quantitative variable</idx><idx>variable!quantitative</idx> for the
            sample we were able to study.  When we write formul√¶ with these values,
            we can't give them variable names that look like <m>a, b, c, \dots</m>, because we
            don't know where to stop (and what would we do if <m>n</m> were more than 26?).
            Instead, we'll use the variables <m>x_1, x_2, \dots, x_n</m> to represent the data
            values.
          </p>

          <p>
            One more very convenient bit of notation, once we have started writing an
            unknown number (<m>n</m>) of numbers <m>x_1, x_2, \dots, x_n</m>, is a way of writing
            their sum:
          </p>

          <definition xml:id="def-summation">
            <idx>summation notation, <m>\Sigma</m></idx>
            <idx><m>\Sigma</m>, summation notation</idx>
            <idx>pig, yellow</idx>
            <statement>
              <p>
                If we have <m>n</m> numbers which we write <m>x_1, \dots, x_n</m>, then we use the
                shorthand <term>summation notation</term> <m>\sum x_i</m> to represent the sum
                <m>\sum x_i = x_1 + \dots + x_n</m>.  <fn>Sometimes you will see this written
                instead <m>\sum_{i=1}^n x_i</m>.  Think of the <q><m>\sum_{i=1}^n{}</m></q>
                as a little computer program which with <m>i=1</m>, increases it one step at a time
                until it gets all the way to <m>i=n</m>, and adds up whatever is to the right.  So,
                for example, <m>\sum_{i=1}^3 2i</m> would be <m>(2*1)+(2*2)+(2*3)</m>, and so has the value <m>12</m>.</fn>
              </p>
            </statement>
          </definition>

          <example xml:id="eg-subscriptssums">
            <statement>
              <p>
                If our dataset were <m>\{1, 2, 17, -3.1415, 3/4\}</m>, then <m>n</m> would be 5 and the
                variables <m>x_1, \dots, x_5</m> would be defined with values <m>x_1=1</m>, <m>x_2=2</m>,
                <m>x_3=17</m>, <m>x_4=-3.1415</m>, and <m>x_5=3/4</m>.
              </p>

              <p>
                In addition<fn>no pun intended</fn>, we would have <m>\sum x_i = x_1+x_2+x_3+x_4+x_5=1+2+17-3.1415+ 3/4=17.6085</m>.
              </p>
            </statement>
          </example>

          <subsection xml:id="ssec-Mode">
            <title>Mode</title>
            <p>
              Let's first discuss probably the simplest measure of central tendency, and in
              fact one which was foreshadowed by terms like <q>unimodal.</q>
            </p>

            <definition xml:id="def-mode">
              <idx>mode</idx>
              <statement>
                <p>
                  A <term>mode</term> of a dataset <m>x_1, \dots, x_n</m> of <m>n</m> numbers is one of the
                  values <m>x_i</m> which occurs at least as often in the dataset as any other value.
                </p>
              </statement>
            </definition>

            <p>
              It would be nice to say this in a simpler way, something like <q>the mode is
              the value which occurs the most often in the dataset,</q> but there may not be
              a single such number.
            </p>

            <example xml:id="eg-mode">
              <statement>
                <p>
                  Continuing with the data from <xref ref="eg-stemandleafplot"/>, it is easy to
                  see, looking at the stem-and-leaf plot, that both 73 and 90 are modes.
                </p>

                <p>
                  Note that in some of the histograms we made using these data and different bin
                  widths, the bins containing 73 and 90 were of the same height, while in others
                  they were of different heights.  This is an example of how it can be quite hard
                  to see on a histogram where the mode is<ellipsis/> or where the <term>modes</term> are.
                </p>
              </statement>
            </example>
          </subsection>

          <subsection xml:id="ssec-Mean">
            <title>Mean</title>
            <p>
              The next measure of central tendency, and certainly the one heard most often
              in the press, is simply the average.  However, in statistics, this is given a
              different name.
            </p>

            <definition xml:id="def-mean">
              <idx>mean</idx>
              <idx>average!see: {mean}</idx>
              <statement>
                <p>
                  The <term>mean</term> of a dataset <m>x_1, \dots, x_n</m> of <m>n</m> numbers is given by the
                  formula <m>\left(\sum x_i\right)/n</m>.
                </p>

                <p>
                  If the data come from a sample, we use the notation
                  <m>\overline{x}</m><idx><m>\overline{x}</m>, sample mean</idx> for the
                  <term>sample mean</term><idx>sample mean, <m>\overline{x}</m></idx><idx>mean!sample</idx>.
                </p>

                <p>
                  If <m>\{x_1, \dots, x_n\}</m> is all of the data from an entire population, we use
                  the notation <m>\mu_X</m> [this is the Greek letter <q>mu,</q> pronounced <q>mew,</q> to
                  rhyme with <q>new.</q>] for the
                  <term>population mean</term><idx>population mean, <m>\mu_X</m></idx><idx>mean!population</idx><idx><m>\mu_X</m>, population mean</idx>.
                </p>
              </statement>
            </definition>

            <example xml:id="eg-mean1">
              <statement>
                <p>
                  Since we've already computed the sum of the data in
                  <xref ref="eg-subscriptssums"/> to be <m>17.6085</m> and there were <m>5</m> values in
                  the dataset, the mean is <m>\overline{x}=17.6085/5 = 3.5217</m><idx>sample mean, <m>\overline{x}</m></idx><idx>mean!sample</idx><idx><m>\overline{x}</m>, sample mean</idx>.
                </p>
              </statement>
            </example>

            <example xml:id="eg-mean2">
              <statement>
                <p>
                  Again using the data from <xref ref="eg-stemandleafplot"/>, we can calculate
                  the mean <m>\overline{x}=\left(\sum x_i\right)/n =2246/30=74.8667</m><idx>sample mean, <m>\overline{x}</m></idx><idx>mean!sample</idx><idx><m>\overline{x}</m>, sample mean</idx>.
                </p>
              </statement>
            </example>

            <p>
              Notice that the mean in the two examples above was not one of the data values.
              This is true quite often.  What that means is that the phrase <q>the average
              <em>whatever</em>,</q> as in <q>the average American family has <m>X</m></q> or <q>the
              average student does <m>Y</m>,</q> is not talking about any particular family, and
              we should not expect any particular family or student to have or do that
              thing.  Someone with a statistical education should mentally edit every
              phrase like that they hear to be instead something like <q>the mean of the
              variable <m>X</m> on the population of all American families is ...,</q> or <q>the
              mean of the variable <m>Y</m> on the population of all students is ...,</q> or
              whatever.
            </p>
          </subsection>

          <subsection xml:id="ssec-Median">
            <title>Median</title>
            <p>
              Our third measure of central tendency is not the result of arithmetic, but
              instead of putting the data values in increasing order.
            </p>

            <definition xml:id="def-median">
              <idx>median</idx>
              <statement>
                <p>
                  Imagine that we have put the values of a dataset <m>\{x_1, \dots, x_n\}</m> of <m>n</m>
                  numbers in increasing (or at least non-decreasing) order, so that
                  <m>x_1\le x_2\le \dots \le x_n</m>.  Then if <m>n</m> is odd, the <term>median</term> of
                  the dataset is the middle value, <m>x_{(n+1)/2}</m>, while if <m>n</m> is even,
                  the median is the mean of the two middle numbers,
                  <m>\frac{x_{n/2}+x_{(n/2)+1}}{2}</m>.
                </p>
              </statement>
            </definition>

            <example xml:id="eg-median1">
              <statement>
                <p>
                  Working with the data in <xref ref="eg-subscriptssums"/>, we must first put
                  them in order, as <m>\{-3.1415, 3/4, 1, 2, 17\}</m>, so the median of this
                  dataset is the middle value, <m>1</m>.
                </p>
              </statement>
            </example>

            <example xml:id="eg-median2">
              <statement>
                <p>
                  Now let us find the median of the data from <xref ref="eg-stemandleafplot"/>.
                  Fortunately, in that example, we made a stem-and-leaf plot and even put the
                  leaves in order, so that starting at the bottom and going along the rows of
                  leaves and then up to the next row, will give us all the values in order!
                  Since there are 30 values, we count up to the <m>15^{th}</m> and <m>16^{th}</m> values,
                  being 76 and 77, and from this we find that the median of the dataset is
                  <m>\frac{76+77}{2}=76.5</m>.
                </p>
              </statement>
            </example>
          </subsection>

          <subsection xml:id="ssec-SaWoTMoCT">
            <title>Strengths and Weaknesses of These Measures of Central Tendency</title>
            <p>
              The weakest of the three measures above is the mode<idx>mode</idx>.  Yes, it is
              nice to know which value happened most often in a dataset (or which values all
              happened equally often and more often then all other values).  But this often
              does not necessarily tell us much about the over-all structure of the data.
            </p>

            <example xml:id="eg-modeweak">
              <idx>mode</idx>
              <statement>
                <p>
                  Suppose we had the data
                  <me>
                    \begin{matrix}
                    86 &amp; 80 &amp; 25 &amp; 77 &amp; 73 &amp; 76 &amp; 100 &amp; 90 &amp; 67 &amp; 93\\
                    94 &amp; 83 &amp; 72 &amp; 75 &amp; 79 &amp; 70 &amp; 91 &amp; 82 &amp; 71 &amp; 95\\
                    40 &amp; 58 &amp; 68 &amp; 69 &amp; 100 &amp; 78 &amp; 87 &amp; 25 &amp; 92 &amp; 74
                    \end{matrix}
                  </me>
                  with corresponding stem-and-leaf plot
                </p>

                <p>
                  [PLACEHOLDER: Table with stem-and-leaf plot showing stems 2-10 with leaves]
                </p>

                <p>
                  This would have a histogram with bins of width 10 that looks exactly like the
                  one in <xref ref="eg-scoreshistbytens"/> <mdash/> so the center of the histogram
                  would seem, visually, still to be around the bar over the 80s <mdash/> but now
                  there is a unique mode of 25.
                </p>
              </statement>
            </example>

            <p>
              What this example shows is that a small change in some of the data values,
              small enough not to change the histogram at all, can change the mode(s)
              drastically.  It also shows that the location of the mode says very little
              about the data in general or its shape, the mode is based entirely on a
              possibly accidental coincidence of some values in the dataset, no matter if
              those values are in the <q>center</q> of the histogram or not.
            </p>

            <p>
              The mean<idx>mean</idx> has a similar problem: a small change in the data, in the
              sense of adding only one new data value, but one which is very far away from
              the others, can change the mean quite a bit.  Here is an example.
            </p>

            <example xml:id="eg-mean3">
              <statement>
                <p>
                  Suppose we take the data from <xref ref="eg-stemandleafplot"/> but change only
                  one value <mdash/> such as by changing the 100 to a 1000, perhaps by a simple typo
                  of the data entry.  Then if we calculate the mean, we get
                  <m>\overline{x}=\left(\sum x_i\right)/n =3146/30=104.8667</m><idx>sample mean, <m>\overline{x}</m></idx><idx>mean!sample</idx><idx><m>\overline{x}</m>, sample mean</idx>, which is quite
                  different from the mean of original dataset.
                </p>
              </statement>
            </example>

            <p>
              A data value which seems to be quite different from all (or the great majority
              of) the rest is called an <em>outlier</em><idx>outlier</idx><fn>This is a very
              informal definition of an outlier.  Below we will have an extremely precise
              one.</fn>  What we have just seen is that
              <term>the mean is very sensitive to outliers</term><idx>sensitive to outliers</idx>.
              This is a serious defect, although otherwise it is easy to compute, to work
              with, and to prove theorems about.
            </p>

            <p>
              Finally, the median<idx>median</idx> is somewhat tedious to compute, because
              the first step is to put all the data values in order, which can be very
              time-consuming.  But, once that is done, throwing in an outlier tends to move
              the median only a little bit.  Here is an example.
            </p>

            <example xml:id="eg-median3">
              <statement>
                <p>
                  If we do as in <xref ref="eg-mean3"/> and change the data value of 100 in the
                  dataset of <xref ref="eg-stemandleafplot"/> to 1000, but leave all of the other
                  data values unchanged, it does not change the median at all since the 1000 is
                  the new largest value, and that does not change the two middle values at all.
                </p>

                <p>
                  If instead we take the data of <xref ref="eg-stemandleafplot"/> and simply add
                  another value, 1000, without taking away the 100, that does change the median:
                  there are now an odd number of data values, so the median is the middle one
                  after they are put in order, which is 78.  So the median has changed by only
                  half a point, from 77.5 to 78.   And this would even be true if the value we
                  were adding to the dataset were 1000000 and not just 1000!
                </p>
              </statement>
            </example>

            <p>
              In other words,
              <term>the median is very insensitive to outliers</term><idx>insensitive to outliers</idx>.
              Since, in practice, it is very easy for datasets to have a few random, bad
              values (typos, mechanical errors, <em>etc.</em>), which are often outliers, it
              is usually smarter to use the median than the mean.
            </p>

            <p>
              As one final point, note that as we mentioned in <xref ref="ssec-Mean"/>, the
              word <q>average,</q> the unsophisticated version of <q>mean,</q> is often incorrectly
              used as a modifier of the individuals in some population being studied (as in
              <q>the average American ...</q>), rather than as a modifier of the variable in
              the study (<q>the average income...</q>), indicating a fundamental misunderstanding
              of what the mean <em>means</em>.  If you look a little harder at this
              misunderstanding, though, perhaps it is based on the idea that we are looking
              for the center, the <q>typical</q> value of the variable.
            </p>

            <p>
              The mode might seem like a good way <mdash/> it's the most frequently occurring
              value.  But we have seen how that is somewhat flawed.
            </p>

            <p>
              The mean might also seem like a good way <mdash/> it's the <q>average,</q> literally.
              But we've also seen problems with the mean.
            </p>

            <p>
              In fact, the median is probably closest to the intuitive idea of <q>the center
              of the data.</q>  It is, after all, a value with the property that both above
              and below that value lie half of the data values.
            </p>

            <p>
              One last example to underline this idea:
            </p>

            <example xml:id="eg-meanmedianincome">
              <idx>mean</idx>
              <idx>median</idx>
              <idx>income distribution</idx>
              <idx>Great Recession</idx>
              <statement>
                <p>
                  The period of economic difficulty for world markets in the late 2000s and early
                  2010s is sometimes called the <term>Great Recession</term>.  Suppose a politician says
                  that we have come out of that time of troubles, and gives as proof the fact
                  that the average family income has increased from the low value it had during
                  the Great Recession back to the values it had before then, and perhaps is even
                  higher than it was in 2005.
                </p>

                <p>
                  It is possible that in fact people are better off, as the increase in this
                  average <mdash/> mean <mdash/> seems to imply.  But it is also possible that while the mean
                  income has gone up, the <em>median</em> income is still low.  This would happen
                  if the histogram of incomes recently still has most of the tall bars down
                  where the variable (family income) is low, but has a few, very high outliers.
                  In short, if the super-rich have gotten even super-richer, that will make the
                  mean (average) go up, even if most of the population has experienced stagnant
                  or decreasing wages <mdash/> but the median will tell what is happening to most of
                  the population.
                </p>

                <p>
                  So when a politician uses the evidence of the average (mean) as suggested here,
                  it is possible they are trying to hide from the public the reality of what is
                  happening to the rich and the not-so-rich.  It is also possible that this
                  politician is simply poorly educated in statistics and doesn't realize what is
                  going on.  You be the judge ... but pay attention so you know what to ask about.
                </p>
              </statement>
            </example>

            <p>
              The last thing we need to say about the strengths and weaknesses of our
              different measures of central tendency is a way to use the weaknesses of the
              mean and median to our advantage.  That is, since the mean is sensitive to
              outliers<idx>sensitive to outliers</idx>, and pulled in the direction of those outliers, while the median is
              not, we can use the difference between the two to tell us which way a histogram
              is skewed.
            </p>

            <p>
              <term>Fact:</term> If the mean of a dataset is larger than the median, then histograms of that
              dataset will be right-skewed<idx>right-skewed histogram, dataset, or distribution</idx><idx>skewed histogram, dataset, or distribution!right</idx>.  Similarly, if the mean is less than the median,
              histograms will be left-skewed<idx>left-skewed histogram, dataset, or distribution</idx><idx>skewed histogram, dataset, or distribution!left</idx>.
              <idx>mean</idx><idx>median</idx><idx>skewed histogram, dataset, or distribution</idx>
            </p>
          </subsection>
        </section>
        <section xml:id="sec-NDoDIMoS">
          <title>Numerical Descriptions of Data, II: Measures of Spread</title>
          <subsection xml:id="ssec-Range">
            <title>Range</title>
            <p>
              The simplest<mdash/>and least useful<mdash/>measure of the spread<idx>spread of a histogram, dataset, or distribution</idx> of
              some data is literally how much space on the <m>x</m>-axis the histogram takes up.
              To define this, first a bit of convenient notation:
            </p>

            <definition xml:id="def-xminxmax">
              <idx><h>notation</h><h><m>x_{min}</m>, minimum value in dataset</h></idx>
              <idx><h>notation</h><h><m>x_{max}</m>, maximum value in dataset</h></idx>
              <statement>
                <p>
                  Suppose <m>x_1, \dots, x_n</m> is some quantitative dataset.  We shall write
                  <m>x_{min}</m> for the smallest and <m>x_{max}</m> for the largest values in the dataset.
                </p>
              </statement>
            </definition>

            <p>
              With this, we can define our first measure of spread<idx>spread of a histogram, dataset, or distribution</idx>
            </p>

            <definition xml:id="def-range">
              <idx>range</idx>
              <statement>
                <p>
                  Suppose <m>x_1, \dots, x_n</m> is some quantitative dataset.  The <term>range</term> of
                  this data is the number <m>x_{max}-x_{min}</m>.
                </p>
              </statement>
            </definition>

            <example xml:id="eg-spread1">
              <statement>
                <p>
                  Using again the statistics test scores data from
                  <xref ref="eg-stemandleafplot"/>, we can read off from the stem-and-leaf plot
                  that <m>x_{min}=25</m> and <m>x_{max}=100</m>, so the range is <m>75(=100-25)</m>.
                </p>
              </statement>
            </example>

            <example xml:id="eg-spread2">
              <statement>
                <p>
                  Working now with the made-up data in <xref ref="eg-subscriptssums"/>, which
                  was put into increasing order in <xref ref="eg-median1"/>, we can see that
                  <m>x_{min}=-3.1415</m> and <m>x_{max}=17</m>, so the range is <m>20.1415(=17-(-3.1415))</m>.
                </p>
              </statement>
            </example>

            <p>
              The thing to notice here is that since the idea of outliers is that they are
              outside of the normal behavior of the dataset, if there are any outliers they
              will definitely be what value gets called <m>x_{min}</m> or <m>x_{max}</m> (or both).  So
              <term>the range is supremely sensitive to outliers</term><idx>sensitive to outliers</idx>: if there are any outliers,
              the range will be determined exactly by them, and not by what the typical data
              is doing.
            </p>
          </subsection>
          <subsection xml:id="ssec-QuartilesIQR">
            <title>Quartiles and the <m>IQR</m></title>
            <p>
              Let's try to find a substitute for the range which is not so sensitive to
              outliers.  We want to see how far apart not the maximum and minimum of the
              whole dataset are, but instead how far apart are the typical larger
              values in the dataset and the typical smaller values.  How can we measure
              these typical larger and smaller?  One way is to define these in terms of the
              typical<mdash/>central<mdash/>value of the upper half of the data and the typical
              value of the lower half of the data.  Here is the definition we shall use for
              that concept:
            </p>

            <definition xml:id="def-quartile">
              <idx>quartile</idx>
              <idx>upper half data</idx>
              <idx>lower half data</idx>
              <statement>
                <p>
                  Imagine that we have put the values of a dataset <m>\{x_1, \dots, x_n\}</m> of <m>n</m>
                  numbers in increasing (or at least non-decreasing) order, so that <m>x_1\le
                  x_2\le \dots \le x_n</m>.  If <m>n</m> is odd, call the
                  <term>lower half data</term><idx>lower half data</idx> all the values
                  <m>\{x_1, \dots, x_{(n-1)/2}\}</m> and the <term>upper half data</term><idx>upper half data</idx>
                  all the values <m>\{x_{(n+3)/2}, \dots, x_n\}</m>; if <m>n</m> is even, the
                  <term>lower half data</term> will be the values <m>\{x_1, \dots, x_{n/2}\}</m> and the
                  <term>upper half data</term> all the values <m>\{x_{(n/2)+1}, \dots, x_n\}</m>.
                </p>

                <p>
                  Then the <term>first quartile</term><idx>first quartile</idx><idx>quartile</idx>, written
                  <m>Q_1</m><idx><h>notation</h><h><m>Q_1</m>, first quartile</h></idx>, is the median of the lower half data,
                  and the <term>third quartile</term><idx>third quartile</idx><idx>quartile</idx>, written
                  <m>Q_3</m><idx><h>notation</h><h><m>Q_3</m>, third quartile</h></idx>, is the median of the upper half data.
                </p>
              </statement>
            </definition>

            <p>
              Note that the first quartile is halfway through the lower half of the data.
              In other words, it is a value such that one quarter of the data is smaller.
              Similarly, the third quartile is halfway through the upper half of the data,
              so it is a value such that three quarters of the data is small. Hence the
              names <q>first</q> and <q>third quartiles.</q>
            </p>

            <p>
              We can build a outlier-insensitive<idx>insensitive to outliers</idx> measure of spread<idx>spread of a histogram, dataset, or distribution</idx> out of the
              quartiles.
            </p>

            <definition xml:id="def-IQR">
              <idx><h>notation</h><h><m>IQR</m>, inter-quartile range</h></idx>
              <idx>inter-quartile range, <m>IQR</m></idx>
              <statement>
                <p>
                  Given a quantitative dataset, its <term>inter-quartile range</term> or <term><m>IQR</m></term> is
                  defined by <m>IQR=Q_3-Q_1</m>.
                </p>
              </statement>
            </definition>

            <example xml:id="eg-iqr1">
              <statement>
                <p>
                  Yet again working with the statistics test scores data from
                  <xref ref="eg-stemandleafplot"/>, we can count off the lower and upper half
                  datasets from the stem-and-leaf plot, being respectively
                  <me>
                    \text{Lower}=\{25, 25, 40, 58, 68, 69, 69, 70, 70, 71, 73, 73, 73, 74, 76\}
                  </me>
                  and
                  <me>
                    \text{Upper} = \{77, 78, 80, 83, 83, 86, 87, 88, 90, 90, 90, 92, 93, 95, 100\}\ .
                  </me>
                  It follows that, for these data, <m>Q_1=70</m> and <m>Q_3=88</m>, so <m>IQR=18(=88-70)</m>.
                </p>
              </statement>
            </example>

            <example xml:id="eg-iqr2">
              <statement>
                <p>
                  Working again with the made-up data in <xref ref="eg-subscriptssums"/>, which
                  was put into increasing order in <xref ref="eg-median1"/>, we can see that the
                  lower half data is <m>\{-3.1415, .75\}</m>, the upper half is
                  <m>\{2, 17\}</m>, <m>Q_1=-1.19575(=\frac{-3.1415+.75}{2})</m>, <m>Q_3=9.5(=\frac{2+17}{2})</m>,
                  and <m>IQR=10.69575(=9.5-(-1.19575))</m>.
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-VarStdDev">
            <title>Variance and Standard Deviation</title>
            <p>
              We've seen a crude measure of spread<idx>spread of a histogram, dataset, or distribution</idx>, like the crude measure
              <q>mode</q><idx>mode</idx> of central tendency.  We've also seen a better measure
              of spread, the <m>IQR</m><idx><h>notation</h><h><m>IQR</m>, inter-quartile range</h></idx>, which is
              insensitive<idx>insensitive to outliers</idx> to outliers like the median<idx>median</idx> (and built out of
              medians).  It seems that, to fill out the parallel triple of measures, there
              should be a measure of spread which is similar to the mean.  Let's try to
              build one.
            </p>

            <p>
              Suppose the data is sample data.  Then how far a particular data value <m>x_i</m>
              is from the sample mean <m>\overline{x}</m><idx>sample mean, <m>\overline{x}</m></idx><idx><h>mean</h><h>sample</h></idx><idx><h>notation</h><h><m>\overline{x}</m>, sample mean</h></idx>
              is just <m>x_i-\overline{x}</m>.  So the mean displacement from the mean, the mean
              of <m>x_i-\overline{x}</m>, should be a good measure of variability, shouldn't it?
            </p>

            <p>
              Unfortunately, it turns out that the mean of
              <m>x_i-\overline{x}</m><idx>sample mean, <m>\overline{x}</m></idx><idx><h>mean</h><h>sample</h></idx><idx><h>notation</h><h><m>\overline{x}</m>, sample mean</h></idx> is always 0.  This
              is because when <m>x_i>\overline{x}</m>, <m>x_i-\overline{x}</m> is positive, while
              when <m>x_i&lt;\overline{x}</m>, <m>x_i-\overline{x}</m> is negative, and it turns out
              that the positives always exactly cancel the negatives (see if you can prove
              this algebraically, it's not hard).
            </p>

            <p>
              We therefore need to make the numbers
              <m>x_i-\overline{x}</m><idx>sample mean, <m>\overline{x}</m></idx><idx><h>mean</h><h>sample</h></idx><idx><h>notation</h><h><m>\overline{x}</m>, sample mean</h></idx> positive before
              taking their mean.  One way to do this is to square them all.  Then we take
              something which is almost the mean of these squared numbers to get another
              measure of spread<idx>spread of a histogram, dataset, or distribution</idx> or variability:
            </p>

            <definition xml:id="def-varstddevsamp">
              <idx>variance</idx>
              <idx>sample variance, <m>S_x^2</m></idx>
              <idx><h>notation</h><h><m>S_x^2</m>, sample variance</h></idx>
              <idx><h>notation</h><h><m>S_x</m>, sample standard deviation</h></idx>
              <idx>standard deviation</idx>
              <idx>sample standard deviation, <m>S_x</m></idx>
              <statement>
                <p>
                  Given sample data <m>x_1, \dots, x_n</m> from a sample of size
                  <m>n</m>, the <term>sample variance</term> is defined as
                  <me>
                    S_x^2 = \frac{\sum \left(x_i-\overline{x}\right)^2}{n-1} .
                  </me>
                  <idx><h>notation</h><h><m>\overline{x}</m>, sample mean</h></idx>
                  Out of this, we then define the <term>sample standard deviation</term>
                  <me>
                    S_x = \sqrt{S_x^2} = \sqrt{\frac{\sum \left(x_i-\overline{x}\right)^2}{n-1}} .
                  </me>
                  <idx><h>notation</h><h><m>\overline{x}</m>, sample mean</h></idx>
                </p>
              </statement>
            </definition>

            <p>
              Why do we take the square root in that sample standard deviation?  The answer
              is that the measure we build should have the property that if all the numbers
              are made twice as big, then the measure of spread<idx>spread of a histogram, dataset, or distribution</idx> should also be
              twice as big.  Or, for example, if we first started working with data measured
              in feet and then at some point decided to work in inches, the numbers would
              all be 12 times as big, and it would make sense if the measure of spread were
              also 12 times as big.
            </p>

            <p>
              The variance does not have this property: if the data are all doubled, the
              variance increases by a factor of 4.  Or if the data are all multiplied by 12,
              the variance is multiplied by a factor of 144.
            </p>

            <p>
              If we take the square root of the variance, though, we get back to the nice
              property of doubling data doubles the measure of spread<idx>spread of a histogram, dataset, or distribution</idx>,
              <em>etc.</em>  For this reason, while we have defined the variance on its own
              and some calculators, computers, and on-line tools will tell the variance
              whenever you ask them to computer 1-variable statistics, we will in this
              class only consider the variance a stepping stone on the way to the real
              measure of spread of data, the standard deviation.
            </p>

            <p>
              One last thing we should define in this section.  For technical reasons that
              we shall not go into now, the definition of standard deviation is slightly
              different if we are working with population data and not sample data:
            </p>

            <definition xml:id="def-varstddevpop">
              <idx>variance</idx>
              <idx>population variance, <m>\sigma_X^2</m></idx>
              <idx><h>notation</h><h><m>\sigma_X^2</m>, population variance</h></idx>
              <idx><h>notation</h><h><m>\sigma_X</m>, population standard deviation</h></idx>
              <idx>standard deviation</idx>
              <idx>population standard deviation, <m>\sigma_X</m></idx>
              <statement>
                <p>
                  Given data <m>x_1, \dots, x_n</m> from an entire population of size <m>n</m>, the
                  <term>population variance</term> is defined as
                  <me>
                    \sigma_X^2 = \frac{\sum \left(x_i-\mu_X\right)^2}{n} .
                  </me>
                  Out of this, we then define the <term>population standard deviation</term>
                  <me>
                    \sigma_X = \sqrt{\sigma_X^2} =
                    \sqrt{\frac{\sum \left(x_i-\mu_X\right)^2}{n}} .
                  </me>
                </p>
              </statement>
            </definition>

            <p>
              [This letter <m>\sigma</m> is the lower-case Greek letter sigma, whose upper case
              <m>\Sigma</m> you've seen elsewhere.]
            </p>

            <p>
              Now for some examples.  Notice that to calculate these values, we shall
              always use an electronic tool like a calculator<idx>calculator</idx> or a
              spreadsheet<idx><h>spreadsheet</h><see><term>LibreOffice Calc</term> and <term>MS Excel</term></see></idx>
              that has a built-in variance<idx>variance</idx> and standard
              deviation<idx>standard deviation</idx> program<mdash/>experience shows that it is
              nearly impossible to get all the calculations entered correctly into a
              non-statistical calculator, so we shall not even try.
            </p>

            <example xml:id="eg-varstd1">
              <statement>
                <p>
                  For the statistics test scores data from <xref ref="eg-stemandleafplot"/>,
                  entering them into a spreadsheet and using <c>VAR.S</c><idx><h>VAR.S</h><h>sample variance in spreadsheets</h></idx> and
                  <c>STDEV.S</c><idx><h>STDEV.S</h><h>sample standard deviation in spreadsheets</h></idx> for
                  the sample variance and standard deviation<idx><h>notation</h><h><m>S_x</m>, sample standard deviation</h></idx><idx>standard deviation</idx><idx>sample standard deviation, <m>S_x</m></idx><idx>sample variance, <m>S_x^2</m></idx><idx><h>notation</h><h><m>S_x^2</m>, sample variance</h></idx> and
                  <c>VAR.P</c><idx><h>VAR.P</h><h>population variance in spreadsheets</h></idx> and <c>STDEV.P</c><idx><h>STDEV.P</h><h>population standard deviation in spreadsheets</h></idx>
                  for population variance and population standard deviation<idx>variance</idx><idx>population variance, <m>\sigma_X^2</m></idx><idx><h>notation</h><h><m>\sigma_X^2</m>, population variance</h></idx><idx><h>notation</h><h><m>\sigma_X</m>, population standard deviation</h></idx><idx>standard deviation</idx><idx>population standard deviation, <m>\sigma_X</m></idx>, we get
                  <md>
                    <mrow>S_x^2 \amp = 331.98</mrow>
                    <mrow>S_x \amp = 18.22</mrow>
                    <mrow>\sigma_X^2 \amp = 330.92</mrow>
                    <mrow>\sigma_X \amp = 17.91</mrow>
                  </md>
                </p>
              </statement>
            </example>

            <example xml:id="eg-varstd2">
              <statement>
                <p>
                  Similarly, for the data in <xref ref="eg-subscriptssums"/>, we find in the same
                  way that
                  <md>
                    <mrow>S_x^2 \amp = 60.60</mrow>
                    <mrow>S_x \amp = 7.78</mrow>
                    <mrow>\sigma_X^2 \amp = 48.48</mrow>
                    <mrow>\sigma_X \amp = 6.96</mrow>
                  </md>
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-SaWoTMoS">
            <title>Strengths and Weaknesses of These Measures of Spread</title>
            <p>
              We have already said that
              <term>the range<idx>range</idx> is extremely sensitive to outliers</term><idx>sensitive to outliers</idx>.<idx>outlier</idx>
            </p>

            <p>
              The <m>IQR</m>,<idx><h>notation</h><h><m>IQR</m>, inter-quartile range</h></idx><idx>inter-quartile range, <m>IQR</m></idx>
              however, is built up out of medians<idx>median</idx>, used in different ways, so
              <term>the <m>IQR</m> is insensitive to outliers</term><idx>insensitive to outliers</idx>.<idx>outlier</idx>
            </p>

            <p>
              The variance<idx>variance</idx>, both sample and population, is built using a
              process quite like a mean<idx>mean</idx>, and in fact also has the mean itself
              in the defining formula.  Since the standard deviation<idx>standard deviation</idx>
              in both cases is simply the square root of the variance, it follows that
              <term>the sample and population variances and standard deviations
              are all sensitive to outliers</term>.<idx>sensitive to outliers</idx><idx>outlier</idx>
            </p>

            <p>
              This differing sensitivity and insensitivity to outliers<idx>outlier</idx> is the
              main difference between the different measures of spread<idx>spread of a histogram, dataset, or distribution</idx> that we
              have discussed in this section.
            </p>

            <p>
              One other weakness, in a certain sense, of the
              <m>IQR</m><idx><h>notation</h><h><m>IQR</m>, inter-quartile range</h></idx> is that there are several
              different definitions in use of the quartiles, based upon whether the median
              value is included or not when dividing up the data.  These are called, for
              example, <c>QUARTILE.INC</c><idx><h>QUARTILE.INC</h><h>quartile computation in spreadsheets</h></idx> and <c>QUARTILE.EXC</c><idx><h>QUARTILE.EXC</h><h>quartile computation in spreadsheets</h></idx> on some spreadsheets.  It
              can then be confusing which one to use.
            </p>
          </subsection>
          <subsection xml:id="ssec-AFDoO">
            <title>A Formal Definition of Outliers<mdash/>the <m>1.5\,IQR</m> Rule</title>
            <p>
              So far, we have said that outliers<idx>outlier</idx> are simply data that are
              <em>atypical</em>.  We need a precise definition that can be carefully checked.
              What we will use is a formula (well, actually two formul√¶) that describe
              that idea of an outlier being <em>far away from the rest of data</em>.
            </p>

            <p>
              Actually, since outliers should be far away either in being significantly
              bigger than the rest of the data or in being significantly smaller, we should
              take a value on the upper side of the rest of the data, and another on the
              lower side, as the starting points for this <em>far away</em>.  We can't pick
              the <m>x_{max}</m><idx><h>notation</h><h><m>x_{max}</m>, maximum value in dataset</h></idx> and
              <m>x_{min}</m><idx><h>notation</h><h><m>x_{min}</m>, minimum value in dataset</h></idx> as those starting
              points, since they will be the outliers themselves, as we have noticed.  So
              we will use our earlier idea of a value which is typical for the larger part
              of the data, the quartile<idx>quartile</idx> <m>Q_3</m><idx><h>notation</h><h><m>Q_3</m>, third quartile</h></idx>,
              and <m>Q_1</m><idx><h>notation</h><h><m>Q_1</m>, first quartile</h></idx> for the corresponding
              lower part of the data.
            </p>

            <p>
              Now we need to decide how far is <em>far enough away</em> from those quartiles
              to count as an outlier.  If the data already has a lot of variation, then a
              new data value would have to be quite far in order for us to be sure that it
              is not out there just because of the variation already in the data.  So our
              measure of <em>far enough</em> should be in terms of a measure of
              spread<idx>spread of a histogram, dataset, or distribution</idx> of the data.
            </p>

            <p>
              Looking at the last section, we see that only the
              <m>IQR</m><idx><h>notation</h><h><m>IQR</m>, inter-quartile range</h></idx> is a measure of
              spread<idx>spread of a histogram, dataset, or distribution</idx> which is insensitive to outliers<idx>insensitive to outliers</idx><mdash/>and we definitely
              don't want to use a measure which is sensitive to the outliers<idx>sensitive to outliers</idx>, one which
              would have been affected by the very outliers we are trying to define.
            </p>

            <p>
              All this goes together in the following
            </p>

            <definition xml:id="def-outlier">
              <idx>outlier</idx>
              <idx><m>1.5\,IQR</m> Rule for Outliers</idx>
              <title>The <m>1.5\,IQR</m> Rule for Outliers</title>
              <statement>
                <p>
                  Starting with a quantitative dataset whose first and third
                  quartiles<idx>quartile</idx> are <m>Q_1</m><idx><h>notation</h><h><m>Q_1</m>, first quartile</h></idx> and
                  <m>Q_3</m> and whose inter-quartile range is <m>IQR</m>, a data value <m>x</m> is
                  [officially, from now on] called an <term>outlier</term> if <m>x&lt;Q_1-1.5\,IQR</m> or
                  <m>x>Q_3+1.5\,IQR</m>.
                </p>
              </statement>
            </definition>

            <p>
              Notice this means that <m>x</m> is not an outlier if it satisfies
              <m>Q_1-1.5\,IQR\le x\le Q_3+1.5\,IQR</m>.
            </p>

            <example xml:id="eg-outliers1">
              <statement>
                <p>
                  Let's see if there were any outliers in the test score dataset from
                  <xref ref="eg-stemandleafplot"/>.  We found the quartiles and <m>IQR</m>
                  in <xref ref="eg-iqr1"/>, so from the <m>1.5\,IQR</m> Rule, a data value <m>x</m> will
                  be an outlier if
                  <me>
                    x&lt;Q_1-1.5\,IQR=70-1.5\cdot18=43
                  </me>
                  or if
                  <me>
                    x>Q_3+1.5\,IQR=88+1.5\cdot18=115\ .
                  </me>
                  Looking at the stemplot in <xref ref="eg-stemandleafplot"/>, we conclude that the data
                  values <m>25</m>, <m>25</m>, and <m>40</m> are the outliers in this dataset.
                </p>
              </statement>
            </example>

            <example xml:id="eg-outliers2">
              <statement>
                <p>
                  Applying the same method to the data in <xref ref="eg-subscriptssums"/>, using
                  the quartiles and <m>IQR</m> from <xref ref="eg-iqr2"/>, the condition for an
                  outlier <m>x</m> is
                  <me>
                    x&lt;Q_1-1.5\,IQR=-1.19575-1.5\cdot10.69575=-17.239375
                  </me>
                  or
                  <me>
                    x>Q_3+1.5\,IQR=9.5+1.5\cdot10.69575=25.543625\ .
                  </me>
                  Since none of the data values satisfy either of these conditions, there are
                  no outliers in this dataset.
                </p>
              </statement>
            </example>
          </subsection>
          <subsection xml:id="ssec-TF-NSaB">
            <title>The Five-Number Summary and Boxplots</title>
            <p>
              We have seen that numerical summaries of quantitative data can be very useful
              for quickly understanding (some things about) the data.  It is therefore
              convenient for a nice package of several of these
            </p>

            <definition xml:id="def-fivenumbersum">
              <idx>five-number summary</idx>
              <statement>
                <p>
                  Given a quantitative dataset <m>\{x_1, \dots, x_n\}</m>, the
                  <term>five-number summary</term><fn>Which might write 5N<m>\Sigma</m>ary for short.</fn>
                  of this data is the set of values
                  <me>
                    \left\{x_{min},\ \ Q_1,\ \ \text{median},\ \ Q_3,\ \ x_{max}\right\}
                  </me>
                  <idx><h>notation</h><h><m>x_{min}</m>, minimum value in dataset</h></idx><idx><h>notation</h><h><m>Q_1</m>, first quartile</h></idx><idx>median</idx><idx>quartile</idx><idx><h>notation</h><h><m>Q_3</m>, third quartile</h></idx><idx><h>notation</h><h><m>x_{max}</m>, maximum value in dataset</h></idx>
                </p>
              </statement>
            </definition>

            <example xml:id="eg-5num1">
              <statement>
                <p>
                  Why not write down the five-number summary for the same test score data we
                  saw in <xref ref="eg-stemandleafplot"/>?  We've already done most of the
                  work, such as calculating the min and max in <xref ref="eg-spread1"/>, the
                  quartiles in <xref ref="eg-iqr1"/>, and the median in <xref ref="eg-median2"/>,
                  so the five-number summary is
                  <md>
                    <mrow>x_{min}\amp=25</mrow>
                    <mrow>Q_1\amp=70</mrow>
                    <mrow>\text{median}\amp=76.5</mrow>
                    <mrow>Q_3\amp=88</mrow>
                    <mrow>x_{max}\amp=100</mrow>
                  </md>
                </p>
              </statement>
            </example>

            <example xml:id="eg-5num2">
              <statement>
                <p>
                  And, for completeness, the five number summary for the made-up data in
                  <xref ref="eg-subscriptssums"/> is
                  <md>
                    <mrow>x_{min}\amp=-3.1415</mrow>
                    <mrow>Q_1\amp=-1.19575</mrow>
                    <mrow>\text{median}\amp=1</mrow>
                    <mrow>Q_3\amp=9.5</mrow>
                    <mrow>x_{max}\amp=17</mrow>
                  </md>
                  where we got the min and max from <xref ref="eg-spread2"/>, the median from
                  <xref ref="eg-median1"/>, and the quartiles from <xref ref="eg-iqr2"/>.
                </p>
              </statement>
            </example>

            <p>
              As we have seen already several times, it is nice to have a both a numeric and
              a graphical/visual version of everything.  The graphical equivalent of the
              five-number summary<idx>five-number summary</idx> is
            </p>

            <definition xml:id="def-boxplot">
              <idx>boxplot, box-and-whisker plot</idx>
              <statement>
                <p>
                  Given some quantitative data, a <term>boxplot</term> [sometimes
                  <term>box-and-whisker plot</term>] is a graphical depiction of the five-number
                  summary, as follows:
                </p>
                <ul>
                  <li><p>an axis is drawn, labelled with the variable of the study</p></li>
                  <li><p>tick marks and numbers are put on the axis, enough to allow the
                  following visual features to be located numerically</p></li>
                  <li><p>a rectangle (the <em>box</em>) is drawn parallel to the axis, stretching
                  from values <m>Q_1</m><idx><h>notation</h><h><m>Q_1</m>, first quartile</h></idx> to
                  <m>Q_3</m><idx><h>notation</h><h><m>Q_3</m>, third quartile</h></idx> on the axis</p></li>
                  <li><p>an additional line is drawn, parallel to the sides of the box, 
                  at the axis coordinate of the median<idx>median</idx> of
                  the data</p></li>
                  <li><p>lines are drawn parallel to the axis from the middle of the sides of the box
                  (at the locations <m>Q_1</m> and <m>Q_3</m>)
                  out to the axis coordinates
                  <m>x_{min}</m><idx><h>notation</h><h><m>x_{min}</m>, minimum value in dataset</h></idx> and
                  <m>x_{max}</m><idx><h>notation</h><h><m>x_{max}</m>, maximum value in dataset</h></idx>, where these
                  <em>whiskers</em> terminate in <q>T</q>s.</p></li>
                </ul>
              </statement>
            </definition>

            <example xml:id="eg-boxplot1">
              <statement>
                <p>
                  A boxplot for the test score data we started using in
                  <xref ref="eg-stemandleafplot"/> is easy to make after we found the
                  corresponding five-number summary in <xref ref="eg-5num1"/>:
                </p>
                <p>
                  [PLACEHOLDER: Figure showing boxplot for test score data]
                </p>
              </statement>
            </example>

            <p>
              Sometimes it is nice to make a version of the boxplot which is less sensitive
              to outliers<idx>sensitive to outliers</idx><idx>outlier</idx>.  Since the endpoints of the whiskers are the only
              parts of the boxplot which are sensitive in this way, they are all we have to
              change:
            </p>

            <definition xml:id="def-boxplotwithOLs">
              <idx><h>boxplot, box-and-whisker plot</h><h>showing outliers</h></idx>
              <statement>
                <p>
                  Given some quantitative data, a <term>boxplot showing outliers</term> [sometimes
                  <term>box-and-whisker plot showing outliers</term>] is minor modification of the
                  regular boxplot, as follows
                </p>
                <ul>
                  <li><p>the whiskers only extend as far as the largest and smallest non-outlier
                  data values</p></li>
                  <li><p>dots are put along the lines of the whiskers at the axis coordinates of
                  any outliers in the dataset</p></li>
                </ul>
              </statement>
            </definition>

            <example xml:id="eg-boxplot2">
              <statement>
                <p>
                  A boxplot showing outliers for the test score data we started using in
                  <xref ref="eg-stemandleafplot"/> is only a small modification of the one we
                  just made in <xref ref="eg-boxplot1"/>
                </p>
                <p>
                  [PLACEHOLDER: Figure showing boxplot with outliers for test score data]
                </p>
              </statement>
            </example>
          </subsection>
        </section>
        <section xml:id="sec-one-variable-exercises">
          <title>Exercises</title>
          
          <exercise>
            <statement>
              <p>
                A product development manager at the campus bookstore wants to make sure
                that the backpacks being sold there are strong enough to carry the heavy books
                students carry around campus. The manager decides she will collect some data
                on how heavy are the bags/packs/suitcases students are carrying around at
                the moment, by stopping the next 100 people she meets at the center of campus
                and measuring.
              </p>
              <p>
                What are the individuals in this study? What is the population? Is there a
                sample <mdash/> what is it? What is the variable? What kind of variable is this?
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                During a blood drive on campus, 300 donated blood. Of these, 136 had blood
                of type <m>O</m>, 120 had blood of type <m>A</m>, 32 of type <m>B</m>, and the rest of type
                <m>AB</m>.
              </p>
              <p>
                Answer the same questions as in the previous exercise for this new situation.
              </p>
              <p>
                Now make at least two visual representations of these data.
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Go to <url href="https://en.wikipedia.org/wiki/Heights_of_presidents_and_presidential_candidates_of_the_United_States">the <term>Wikipedia</term> page</url> for <q>Heights of
                Presidents and Presidential Candidates of the United States</q> and look only at
                the heights of the presidents themselves, in centimeters (<em>cm</em>).
              </p>
              <p>
                Make a histogram with these data using bins of width 5. Explain how you are
                handling the edge cases in your histogram.
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Suppose you go to the supermarket every week for a year and buy a bag of
                flour, packaged by a major national flour brand, which is labelled as weighing
                <m>1kg</m>. You take the bag home and weigh it on an extremely accurate scale
                that measures to the nearest <m>{1/100}^{th}</m> of a gram. After the 52 weeks of
                the year of flour buying, you make a histogram of the accurate weights of the
                bags. What do you think that histogram will look like? Will it be symmetric
                or skewed left or right (which one?), where will its center be, will it show
                a lot of variation/spread or only a little? Explain why you think each of
                the things you say.
              </p>
              <p>
                What about if you buy a <m>1kg</m> loaf of bread from the local artisanal bakery
                <mdash/> what would the histogram of the accurate weights of those loaves look like
                (same questions as for histogram of weights of the bags of flour)?
              </p>
              <p>
                If you said that those histograms were symmetric, can you think of a
                measurement you would make in a grocery store or bakery which would be
                skewed; and if you said the histograms for flour and loaf weights were skewed,
                can you think of one which would be symmetric? (Explain why, always, of
                course.) [If you think one of the two above histograms was skewed and one was
                symmetric (with explanation), you don't need to come up with another one here.]
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Twenty sacks of grain weigh a total of <m>1003kg</m>. What is the mean<idx>mean</idx>
                weight per sack?
              </p>
              <p>
                Can you determine the median<idx>median</idx> weight per sack from the given
                information? If so, explain how. If not, give two examples of datasets with
                the same total weight be different medians.
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                For the dataset <m>\{6, -2, 6, 14, -3, 0, 1, 4, 3, 2, 5\}</m>, which we will call
                <m>DS_1</m>, find the mode(s)<idx>mode</idx>, mean<idx>mean</idx>, and
                median<idx>median</idx>.
              </p>
              <p>
                Define <m>DS_2</m> by adding <m>3</m> to each number in <m>DS_1</m>. What are the
                mode(s)<idx>mode</idx>, mean<idx>mean</idx>, and median<idx>median</idx> of <m>DS_2</m>?
              </p>
              <p>
                Now define <m>DS_3</m> by subtracting <m>6</m> from each number in <m>DS_1</m>. What are the
                mode<idx>mode</idx>(s), mean<idx>mean</idx>, and median<idx>median</idx> of <m>DS_3</m>?
              </p>
              <p>
                Next, define <m>DS_4</m> by multiplying every number in <m>DS_1</m> by 2. What are the
                mode<idx>mode</idx>(s), mean<idx>mean</idx>, and median<idx>median</idx> of <m>DS_4</m>?
              </p>
              <p>
                Looking at your answers to the above calculations, how do you think the
                mode<idx>mode</idx>(s), mean<idx>mean</idx>, and median<idx>median</idx> of datasets
                must change when you add, subtract, multiply or divide all the numbers by the
                same constant? Make a specific conjecture!
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                There is a very hard mathematics competition in which college students in the
                US and Canada can participate called the
                <term>William Lowell Putnam Mathematical Competition</term>. It consists of a
                six-hour long test with twelve problems, graded 0 to 10 on each problem, so the
                total score could be anything from 0 to 120.
              </p>
              <p>
                The median<idx>median</idx> score last year on the Putnam exam was 0 (as it often
                is, actually). What does this tell you about the scores of the students who
                took it? Be as precise as you can. Can you tell what fraction (percentage)
                of students had a certain score or scores? Can you figure out what the
                quartiles<idx>quartile</idx> must be?
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Find the range<idx>range</idx>, <m>IQR</m><idx><m>IQR</m>, inter-quartile range</idx>, and
                standard deviation<idx>standard deviation</idx> of the following sample dataset:
              </p>
              <me>
                DS_1 = \{0, 0, 0, 0, 0, .5, 1, 1, 1, 1, 1\}\quad .
              </me>
              <p>
                Now find the range<idx>range</idx>, <m>IQR</m><idx><m>IQR</m>, inter-quartile range</idx>,
                and standard deviation<idx>standard deviation</idx> of the following sample data:
              </p>
              <me>
                DS_2 = \{0, .5, 1, 1, 1, 1, 1, 1, 1, 1, 1\}\quad .
              </me>
              <p>
                Next find the range<idx>range</idx>, <m>IQR</m><idx><m>IQR</m>, inter-quartile range</idx>,
                and standard deviation<idx>standard deviation</idx> of the following sample data:
              </p>
              <me>
                DS_3 = \{0, 0, 0, 0, 0, 0, 0, 0, 0, .5, 1\}\quad .
              </me>
              <p>
                Finally, find the range<idx>range</idx>,
                <m>IQR</m><idx><m>IQR</m>, inter-quartile range</idx>, and standard
                deviation<idx>standard deviation</idx> of sample data <m>DS_4</m>,
                consisting of 98 0s, one .5, and one 1 (so like <m>DS_3</m> except with 0 occurring
                98 times instead of 9 time).
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                What must be true about a dataset if its range<idx>range</idx> is 0? Give the
                most interesting example of a dataset with range of 0 and the property you
                just described that you can think of.
              </p>
              <p>
                What must be true about a dataset if its
                <m>IQR</m><idx><m>IQR</m>, inter-quartile range</idx> is 0? Give the most interesting
                example of a dataset with <m>IQR</m> of 0 and the property you just described that
                you can think of.
              </p>
              <p>
                What must be true about a dataset if its standard
                deviation<idx>standard deviation</idx> is 0? Give the most interesting example
                of a dataset with standard deviation of 0 and the property you just described
                that you can think of.
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Here are some boxplots<idx>boxplot, box-and-whisker plot</idx> of test scores,
                out of 100, on a standardized test given in five different classes <mdash/> the
                same test, different classes. For each of these plots, <m>A - E</m>, describe
                qualitatively (in the sense of ¬ß1.4.2) but in as much detail as
                you can, what must have been the histogram<idx>histogram</idx> for the data
                behind this boxplot. Also sketch a possible such histogram, for each case.
              </p>
              <p>
                [Figure: Boxplot showing test scores for classes A through E - placeholder for converted figure]
              </p>
            </statement>
          </exercise>
          
        </section>
      </chapter>

      <chapter xml:id="chap-bivariate-basics">
        <title>Bi-variate Statistics: Basics</title>
        <section xml:id="sec-TERoID">
          <title>Terminology: Explanatory/Response or Independent/Dependent</title>
          <p>
            All of the discussion so far has been for studies which have a single
            variable.  We may collect the values of this variable for a large population,
            or at least the largest sample we can afford to examine, and we may display
            the resulting data in a variety of graphical ways, and summarize it in a
            variety of numerical ways.  But in the end all this work can only show a
            single characteristic of the individuals.  If, instead, we want to study a
            <em>relationship</em>, we need to collect two (at least) variables and develop
            methods of descriptive statistics which show the relationships between the
            values of these variables.
          </p>
          <p>
            Relationships in data require at least two variables.  While more complex
            relationships can involve more, in this chapter we will start the project of
            understanding <em>bivariate data</em><idx>bivariate data</idx>, data where we make
            two observations for each individual, where we have exactly two variables.
          </p>
          <p>
            If there is a relationship between the two variables we are studying, the
            most that we could hope for would be that that relationship is due to the
            fact that one of the variables <em>causes</em> the other.  In this situation,
            we have special names for these variables
          </p>
          <definition xml:id="def-explanatoryresponsevars">
            <statement>
              <p>
                In a situation with bivariate data, if one variable can take on any value
                without (significant) constraint it is called the <term>independent variable</term><idx>independent variable</idx><idx><h>variable</h><h>independent</h></idx>, while the second
                variable, whose value is (at least partially) controlled by the first, is called
                the <term>dependent variable</term><idx>dependent variable</idx><idx><h>variable</h><h>dependent</h></idx>.
              </p>
              <p>
                Since the value of the dependent variable depends upon the value of the
                independent variable, we could also say that it is explained by the
                independent variable.  Therefore the independent variable is also called the
                <term>explanatory variable</term><idx>explanatory variable</idx><idx><h>variable</h><h>explanatory</h></idx> and the dependent variable is then called the
                <term>response variable</term><idx>response variable</idx><idx><h>variable</h><h>response</h></idx>
              </p>
              <p>
                Whenever we have bivariate data and we have made a choice of which variable
                will be the independent and which the dependent, we write <m>x</m> for the
                independent and <m>y</m> for the dependent variable.
              </p>
            </statement>
          </definition>
          <example xml:id="eg-depindepvars1">
            <statement>
              <p>
                Suppose we have a large warehouse of many different boxes of products ready to
                ship to clients.  Perhaps we have packed all the products in boxes which are
                perfect cubes, because they are stronger and it is easier to stack them
                efficiently.  We could do a study where
              </p>
              <ul>
                <li><p>the <em>individuals</em> would be the boxes of product;</p></li>
                <li><p>the <em>population</em> would be all the boxes in our warehouse;</p></li>
                <li><p>the <em>independent variable</em> would be, for a particular box, the
                length of its side in <em>cm</em>;</p></li>
                <li><p>the <em>dependent variable</em> would be, for a particular box, the cost
                to the customer of buying that item, in US dollars.</p></li>
              </ul>
              <p>
                We might think that the size <em>determines</em> the cost, at least approximately,
                because the larger boxes contain larger products into which went more raw
                materials and more labor, so the items would be more expensive.  So, at least
                roughly, the size may be anything, it is a free or <em>independent</em> choice,
                while the cost is (approximately) determined by the size, so the cost is
                <em>dependent</em>.  Otherwise said, the size <em>explains</em> and the cost is the
                <em>response</em>.  Hence the choice of those variables.
              </p>
            </statement>
          </example>
          <example xml:id="eg-depindepvars3">
            <statement>
              <p>
                Suppose we have exactly the same scenario as above, but now we want to make the
                different choice where
              </p>
              <ul>
                <li><p>the <em>dependent variable</em> would be, for a particular box, the volume
                of that box.</p></li>
              </ul>
            </statement>
          </example>
          <p>
            There is one quite important difference between the two examples above: in
            one case (the cost), knowing the length of the side of a box gives us a hint
            about how much it costs (bigger boxes cost more, smaller boxes cost less) but
            this knowledge is imperfect (sometimes a big box is cheap, sometimes a small
            box is expensive); while in the other case (the volume), knowing the length of
            the side of the box perfectly tells us the volume.  In fact, there is a
            simple geometric formula that the volume <m>V</m> of a cube of side length <m>s</m> is
            given by <m>V=s^3</m>.
          </p>
          <p>
            This motivates a last preliminary definition
          </p>
          <definition xml:id="def-deterministic">
            <idx>deterministic</idx>
            <statement>
              <p>
                We say that the relationship between two variables is <term>deterministic</term> if
                knowing the value of one variable completely determines the value of the
                other.  If, instead, knowing one value does not completely determine the other,
                we say the variables have a
                <term>non-deterministic relationship</term>.<idx>non-deterministic</idx>
              </p>
            </statement>
          </definition>
        </section>
        <section xml:id="sec-scatterplots">
          <title>Scatterplots</title>
          <p>
            When we have bivariate data, the first thing we should always do is draw a
            graph of this data, to get some feeling about what the data is showing us and
            what statistical methods it makes sense to try to use.  The way to do this is
            as follows
          </p>
          <definition xml:id="def-scatterplot">
            <idx>scatterplot</idx>
            <statement>
              <p>
                Given bivariate quantitative data, we make the <term>scatterplot</term> of this
                data as follows:  Draw an <m>x</m>- and a <m>y</m>-axis, and label them with descriptions
                of the independent and dependent variables, respectively.  Then, for each
                individual in the dataset, put a dot on the graph at location <m>(x,y)</m>, if
                <m>x</m> is the value of that individual's independent variable and <m>y</m> the value
                of its dependent variable.
              </p>
            </statement>
          </definition>
          <p>
            After making a scatterplot, we usually describe it qualitatively in three
            respects:
          </p>
          <definition xml:id="def-scattershape">
            <statement>
              <p>
                If the cloud of data points in a scatterplot generally lies near some curve,
                we say that the scatterplot has [approximately] that
                <term>shape</term><idx><h>shape</h><h>scatterplot</h></idx>.
              </p>
              <p>
                A common shape we tend to find in scatterplots is that it is
                <term>linear</term><idx>linear association</idx>
              </p>
              <p>
                If there is no visible shape, we say the scatterplot is
                <term>amorphous</term><idx>amorphous, for scatterplots or associations</idx>, or
                <term>has no clear shape</term>.
              </p>
            </statement>
          </definition>
          <definition xml:id="def-scatterstrength">
            <statement>
              <p>
                When a scatterplot has some visible shape <mdash/> so that we do not describe it as
                amorphous <mdash/> how close the cloud of data points is to that curve is called the
                <term>strength</term><idx>strength of an association</idx> of that association.  In this
                context, a <term>strong</term><idx>strong association</idx> [linear, <em>e.g.,</em>]
                association means that the dots are close to the named curve [line,
                <em>e.g.,</em>], while a <term>weak</term><idx>weak association</idx> association means
                that the points do not lie particularly close to any of the named curves
                [line, <em>e.g.,</em>].
              </p>
            </statement>
          </definition>
          <definition xml:id="def-scatterdirection">
            <statement>
              <p>
                In case a scatterplot has a fairly strong linear association, the
                <term>direction</term><idx>direction of a linear association</idx> of the association
                described whether the line is increasing or decreasing.  We say the
                association is <term>positive</term><idx>positive linear association</idx> if the line
                is increasing and <term>negative</term><idx>negative linear association</idx> if it is
                decreasing.
              </p>
            </statement>
          </definition>
          <p>
            [Note that the words <em>positive</em> and <em>negative</em> here can be thought of
            as describing the <em>slope</em><idx>slope of a line</idx> of the line which we are
            saying is the underlying relationship in the scatterplot.]
          </p>
        </section>
        <section xml:id="sec-correlation">
          <title>Correlation</title>
          <p>
            As before (in ¬ß¬ß<xref ref="sec-NDoDIMotC"/> and <xref ref="sec-NDoDIMoS"/>), when we moved
            from describing histograms with words (like <em>symmetric</em>) to describing them
            with numbers (like the <em>mean</em>), we now will build a numeric measure of
            the strength and direction of a linear association in a scatterplot.
          </p>
          <definition xml:id="def-corrcoeff">
            <idx>correlation coefficient, <m>r</m></idx>
            <idx><h sortby="r">r, correlation coefficient</h></idx>
            <statement>
              <p>
                Given bivariate quantitative data <m>\{(x_1,y_1), \dots , (x_n,y_n)\}</m> the
                <term>[Pearson] correlation coefficient</term>
                of this dataset is
              </p>
              <me>
                r=\frac{1}{n-1}\sum \frac{(x_i-\overline{x})}{s_x}\frac{(y_i-\overline{y})}{s_y}
              </me>
              <p>
                where <m>s_x</m> and <m>s_y</m> are the standard deviations of the <m>x</m> and <m>y</m>,
                respectively, datasets by themselves.
              </p>
            </statement>
          </definition>
          <assemblage xml:id="fact-corrcoefff">
            <title>Fact</title>
            <p>
              For any bivariate quantitative dataset <m>\{(x_1,y_1), \dots ,(x_n,y_n)\}</m> with
              correlation coefficient <m>r</m>, we have
            </p>
            <ol>
              <li><p><m>-1\le r\le 1</m> is always true;</p></li>
              <li><p>if <m>|r|</m> is near <m>1</m> <mdash/> meaning that <m>r</m> is near <m>\pm 1</m> <mdash/> then the
                    linear association between <m>x</m> and <m>y</m> is <em>strong</em></p></li>
              <li><p>if <m>r</m> is near <m>0</m> <mdash/> meaning that <m>r</m> is positive or negative, but near
                    <m>0</m> <mdash/> then the linear association between <m>x</m> and <m>y</m> is <em>weak</em></p></li>
              <li><p>if <m>r>0</m> then the linear association between <m>x</m> and <m>y</m> is positive,
                    while if <m>r&lt;0</m> then the linear association between <m>x</m> and <m>y</m> is
                    negative</p></li>
              <li><p><m>r</m> is the same no matter what units are used for the variables <m>x</m> and
                    <m>y</m> <mdash/> meaning that if we change the units in either variable, <m>r</m> will
                    not change</p></li>
              <li><p><m>r</m> is the same no matter which variable is being used as the explanatory
                    and which as the response variable <mdash/> meaning that if we switch the roles
                    of the <m>x</m> and the <m>y</m> in our dataset, <m>r</m> will not change.</p></li>
            </ol>
          </assemblage>
          <p>
            It is also nice to have some examples of correlation coefficients, such as
          </p>
          <p>
            [Figure: Scatterplot range showing various correlation coefficients - placeholder for converted figure]
          </p>
          <p>
            Many electronic tools which compute the correlation coefficient <m>r</m> of a
            dataset also report its square, <m>r^2</m>.  The reason is explained in the
            following
          </p>
          <assemblage xml:id="fact-rsquared">
            <title>Fact</title>
            <p>
              If <m>r</m> is the correlation coefficient between two variables <m>x</m> and <m>y</m> in some
              quantitative dataset, then its square <m>r^2</m> is the fraction (often described as
              a percentage) of the variation of <m>y</m> which is associated with variation in <m>x</m>.
            </p>
          </assemblage>
          <example xml:id="eg-rsquared">
            <statement>
              <p>
                If the square of the correlation coefficient between the independent variable
                <em>how many hours a week a student studies statistics</em> and the dependent
                variable <em>how many points the student gets on the statistics final exam</em>
                is <m>.64</m>, then 64% of the variation in scores for that class is cause by
                variation in how much the students study.  The remaining 36% of the variation
                in scores is due to other random factors like whether a student was coming
                down with a cold on the day of the final, or happened to sleep poorly the
                night before the final because of neighbors having a party, or some other
                issues different just from studying time.
              </p>
            </statement>
          </example>
        </section>
        <section xml:id="sec-bivariate-exercises">
          <title>Exercises</title>
          <exercise>
            <statement>
              <p>
                Suppose you pick 50 random adults across the United States in
                January 2017 and measure how tall they are.  For each of them, you also get
                accurate information about how tall their (biological) parents are.  Now, using
                as your individuals these 50 adults and as the two variables their heights and
                the average of their parents' heights, make a sketch of what you think the
                resulting scatterplot would look like.  Explain why you made the choice you did
                of one variable to be the explanatory and the other the response variable.
                Tell what are the shape, strength, and direction you see in this scatterplot, if
                it shows a deterministic or non-deterministic association, and why you think
                those conclusions would be true if you were to do this exercise with real data.
              </p>
              <p>
                Is there any time or place other than right now in the United States where you
                think the data you would collect as above would result in a scatterplot that
                would look fairly different in some significant way?  Explain!
              </p>
            </statement>
          </exercise>
          <exercise>
            <statement>
              <p>
                It actually turns out that it is not true that the more a
                person works, the more they produce <ellipsis/> at least not always.  Data on workers
                in a wide variety of industries show that working more hours produces more of
                that business's product for a while, but then after too many hours of work,
                keeping on working makes for almost no additional production.
              </p>
              <p>
                Describe how you might collect data to investigate this relationship, by
                telling what individuals, population, sample, and variables you would use.
                Then, assuming the truth of the above statement about what other research in
                this area has found, make an example of a scatterplot that you think might
                result from your suggested data collection.
              </p>
            </statement>
          </exercise>
          <exercise>
            <statement>
              <p>
                Make a scatterplot of the dataset consisting of the following
                pairs of measurements:
              </p>
              <me>
                \left\{(8,16), (9,9), (10,4), (11,1), (12,0), (13,1), (14,4), (15,9), (16,16)\right\} .
              </me>
              <p>
                You can do this quite easily by hand (there are only nine points!).  Feel free
                to use an electronic device to make the plot for you, if you have one you know
                how to use, but copy the resulting picture into the homework you hand in, either
                by hand or cut-and-paste into an electronic version.
              </p>
              <p>
                Describe the scatterplot, telling what are the shape, strength, and direction.
                What do you think would be the correlation coefficient of this dataset?  As
                always, explain all of your reasoning!
              </p>
            </statement>
          </exercise>
        </section>
      </chapter>

      <chapter xml:id="chap-linear-regression">
        <title>Linear Regression</title>
        
        <p>
          Quick review of equations for lines:
        </p>
        
        <p>
          Recall the equation of a line is usually in the form <m>y=mx+b</m>, where <m>x</m> and
          <m>y</m> are variables and <m>m</m> and <m>b</m> are numbers.  Some basic facts about lines:
        </p>
        
        <ul>
          <li>
            <p>
              If you are given a number for <m>x</m>, you can plug it in to the equation
              <m>y=mx+b</m> to get a number for <m>y</m>, which together give you a point with
              coordinates <m>(x,y)</m> that is on the line.
            </p>
          </li>
          <li>
            <p>
              <m>m</m> is the <em>slope</em><idx>slope of a line</idx>, which tells how much the
              line goes up (increasing <m>y</m>) for every unit you move over to the right
              (increasing <m>x</m>) <mdash/> we often say that the value of the slope is
              <m>m=\frac{rise}{run}</m><idx><h>rise over run</h><see>slope of a line</see></idx>.  The slope can be:
            </p>
            <ul>
              <li>
                <p>
                  <em>positive</em>, if the line is tilted up,
                </p>
              </li>
              <li>
                <p>
                  <em>negative</em>, if the line is tilted down,
                </p>
              </li>
              <li>
                <p>
                  <em>zero</em>, if the line is horizontal, and
                </p>
              </li>
              <li>
                <p>
                  <em>undefined</em>, if the line is vertical.
                </p>
              </li>
            </ul>
          </li>
          <li>
            <p>
              You can calculate the slope by finding the coordinates <m>(x_1,y_1)</m> and
              <m>(x_2,y_2)</m> of any two points on the line and then <m>m=\frac{y_2-y_1}{x_2-x_1}</m>.
            </p>
          </li>
          <li>
            <p>
              In particular, if <m>x_2-x_1=1</m>, then <m>m=\frac{y_2-y_1}{1}=y_2-y_1</m> <mdash/> so
              if you look at how much the line goes up in each step of one unit to the right,
              that number will be the slope <m>m</m> (and if it goes <em>down</em>, the slope <m>m</m> will
              simply be negative).  In other words, the slope answers the question <q>for
              each step to the right, how much does the line increase (or decrease)?</q>
            </p>
          </li>
          <li>
            <p>
              <m>b</m> is the
              <em><m>y</m>-intercept</em><idx><h>y-intercept of a line</h></idx>, which
              tells the <m>y</m>-coordinate of the point where the line crosses the <m>y</m>-axis.
              Another way of saying that is that <m>b</m> is the <m>y</m> value of the line when the
              <m>x</m> is <m>0</m>.
            </p>
          </li>
        </ul>
        
        <section xml:id="sec-TLSRL">
          <title>The Least Squares Regression Line</title>
          
          <p>
            Suppose we have some bivariate quantitative data
            <m>\{(x_1,y_1), \dots , (x_n,y_n)\}</m> for which the correlation coefficient
            indicates some linear association.  It is natural to want to write down
            explicitly the equation of the best line through the data <mdash/> the question is
            what is this line.  The most common meaning given to <em>best</em> in this
            search for the line is <em>the line whose total square error is the smallest
            possible.</em>  We make this notion precise in two steps
          </p>
          
          <definition xml:id="def-residual">
            <idx>residual, for data values and LSRLs</idx>
            <statement>
              <p>
                Given a bivariate quantitative dataset
                <m>\{(x_1,y_1), \dots , (x_n,y_n)\}</m> and a candidate line <m>\widehat{y}=mx+b</m>
                passing through this dataset, a <term>residual</term> is the difference in
                <m>y</m>-coordinates of an actual data point <m>(x_i,y_i)</m> and the line's <m>y</m> value at
                the same <m>x</m>-coordinate.  That is, if the <m>y</m>-coordinate of the line when
                <m>x=x_i</m> is <m>\widehat{y}_i=mx_i+b</m>, then the residual is the measure of error
                given by <m>error_i=y_i-\widehat{y}_i</m>.
              </p>
            </statement>
          </definition>
          
          <p>
            Note we use the convention here and elsewhere of writing
            <m>\widehat{y}</m><idx><m>\widehat{y}</m>, <m>y</m> values on an approximating line</idx>
            for the <m>y</m>-coordinate on an approximating line, while the plain <m>y</m> variable
            is left for actual data values, like <m>y_i</m>.
          </p>
          
          <p>
            Here is an example of what residuals look like
          </p>
          
          <p>
            [Figure: residual.eps - placeholder for residuals illustration]
          </p>
          
          <p>
            Now we are in the position to state the
          </p>
          
          <definition xml:id="def-LSRL">
            <idx>least squares regression line, LSRL</idx>
            <idx>LSRL, least squares regression line</idx>
            <statement>
              <p>
                Given a bivariate quantitative dataset the <term>least square regression line</term>,
                almost always abbreviated to <term>LSRL</term>, is the line for which the sum of the
                squares of the residuals is the smallest possible.
              </p>
            </statement>
          </definition>
          
          <assemblage xml:id="fact-LSRLproperties">
            <title>Fact</title>
            <p>
              If a bivariate quantitative dataset <m>\{(x_1,y_1), \dots , (x_n,y_n)\}</m> has LSRL
              given by <m>\widehat{y}=mx+b</m>, then
            </p>
            <ol>
              <li>
                <p>
                  The slope of the LSRL is given by <m>m=r\frac{s_y}{s_x}</m>, where <m>r</m> is the
                  correlation coefficient of the dataset.
                </p>
              </li>
              <li>
                <p>
                  The LSRL passes through the point <m>(\overline{x},\overline{y})</m><idx><h>sample mean</h><h><m>\overline{x}</m></h></idx><idx><h>mean</h><h>sample</h></idx><idx><m>\overline{x}</m>, sample mean</idx>.
                </p>
              </li>
              <li>
                <p>
                  It follows that the <m>y</m>-intercept of the LSRL is given by
                  <m>b=\overline{y}-\overline{x}\,m=\overline{y}-\overline{x}\,r\,\frac{s_y}{s_x}</m><idx><h>sample mean</h><h><m>\overline{x}</m></h></idx><idx><h>mean</h><h>sample</h></idx><idx><m>\overline{x}</m>, sample mean</idx>.
                </p>
              </li>
            </ol>
          </assemblage>
          
          <p>
            It is possible to find the (coefficients of the) LSRL using the above
            information, but it is often more convenient to use a
            calculator<idx>calculator</idx> or other electronic tool.  Such tools also make
            it very easy to graph the LSRL right on top of the scatterplot <mdash/> although it
            is often fairly easy to sketch what the LSRL will likely look like by just
            making a good guess, using visual intuition, if the linear association is
            strong (as will be indicated by the correlation coefficient).
          </p>
          
          <example xml:id="eg-LSRL">
            <statement>
              <p>
                Here is some data where the individuals are 23 students in a statistics class,
                the independent variable is the students' total score on their homeworks, while
                the dependent variable is their final total course points, both out of 100.
              </p>
              <me>
                \begin{matrix}
                x:\amp 65\amp 65\amp 50\amp 53\amp 59\amp 92\amp 86\amp 84\amp 29\\
                y:\amp 74\amp 71\amp 65\amp 60\amp 83\amp 90\amp 84\amp 88\amp 48\\
                \ \\
                x:\amp 29\amp  9\amp 64\amp 31\amp 69\amp 10\amp 57\amp 81\amp 81\\
                y:\amp 54\amp 25\amp 79\amp 58\amp 81\amp 29\amp 81\amp 94\amp 86\\
                \ \\
                x:\amp 80\amp 70\amp 60\amp 62\amp 59\\
                y:\amp 95\amp 68\amp 69\amp 83\amp 70\\
                \end{matrix}
              </me>
              <p>
                Here is the resulting scatterplot, made with
                <term>LibreOffice Calc</term><idx>LibreOffice Calc</idx><idx><h>Calc</h><h>LibreOffice</h></idx> (a free equivalent of
                <term>Microsoft Excel</term><idx>Microsoft Excel</idx><idx>MS Excel</idx><idx><h>Excel</h><h>Microsoft</h></idx>)
              </p>
              <p>
                [Figure: scatter1.eps - placeholder for scatterplot]
              </p>
              <p>
                It seems pretty clear that there is quite a strong linear association between
                these two variables, as is born out by the correlation coefficient,
                <m>r=.935</m> (computed with <term>LibreOffice Calc</term>'s <c>CORREL</c><idx>CORREL, correlation coefficient in LibreOffice Calc and MS Excel</idx>).  Using then
                <c>STDEV.S</c><idx>STDEV.S, sample standard deviation in spreadsheets</idx> and <c>AVERAGE</c><idx>AVERAGE, sample mean in spreadsheets</idx>, we find that the coefficients of the LSRL for this data,
                <m>\widehat{y}=mx+b</m> are
              </p>
              <me>
                m=r\frac{s_y}{s_x}=.935\frac{18.701}{23.207}=.754\qquad{\rm and}\qquad b=\overline{y}-\overline{x}\,m=71-58\cdot .754=26.976
              </me>
              <p>
                We can also use <term>LibreOffice Calc</term><idx>LibreOffice Calc</idx><idx><h>Calc</h><h>LibreOffice</h></idx>'s <c>Insert Trend Line</c><idx>Insert Trend Line, display LSRL in spreadsheet scatterplots</idx>, with <c>Show Equation</c><idx>Show Equation, display LSRL equation in spreadsheets</idx>, to get all this done
                automatically.  Note that when <term>LibreOffice Calc</term> writes the equation of
                the LSRL, it uses <m>f(x)</m> in place of <m>\widehat{y}</m>, as we would.
              </p>
              <p>
                [Figure: scatter2.eps - placeholder for scatterplot with LSRL]
              </p>
            </statement>
          </example>
          
        </section>
        <section xml:id="sec-AaIoLSRLs">
          <title>Applications and Interpretations of LSRLs</title>
          
          <p>
            Suppose that we have a bivariate quantitative dataset
            <m>\{(x_1,y_1), \dots , (x_n,y_n)\}</m> and we have computed its correlation
            coefficient <m>r</m> and (the coefficients of) its LSRL <m>\widehat{y}=mx+b</m>.
            What is this information good for?
          </p>
          
          <p>
            The main use of the LSRL is described in the following
          </p>
          
          <definition xml:id="def-interpolation">
            <idx>interpolation</idx>
            <statement>
              <p>
                Given a bivariate quantitative dataset and associated LSRL with equation
                <m>\widehat{y}=mx+b</m>, the process of guessing that the value of the dependent
                variable in this relationship to have the value <m>mx_0+b</m>, for <m>x_0</m> any
                value for the independent variable which satisfies <m>x_{min}\le x_0\le x_{max}</m>,
                is called <term>interpolation</term>.
              </p>
            </statement>
          </definition>
          
          <p>
            The idea of interpolation is that we think the LSRL describes as well as
            possible the relationship between the independent and dependent variables, so
            that if we have a new <m>x</m> value, we'll use the LSRL equation to predict what
            would be our best guess of what would be the corresponding <m>y</m>.  Note we
            might have a new value of <m>x</m> because we simply lost part of our dataset and
            are trying to fill it in as best we can.  Another reason might be that a new
            individual came along whose value of the independent variable, <m>x_0</m>, was
            typical of the rest of the dataset <mdash/> so the the very least
            <m>x_{min}\le x_0\le x_{max}</m> <mdash/> and we want to guess what will be the value of the
            dependent variable for this individual before we measure it.  (Or maybe we
            cannot measure it for some reason.)
          </p>
          
          <p>
            A common (but naive) alternate approach to interpolation for a value <m>x_0</m> as
            above might be to find two values <m>x_i</m> and <m>x_j</m> in the dataset which were
            as close to <m>x_0</m> as possible, and on either side of it (so <m>x_i\lt x_0\lt x_j</m>),
            and simply to guess that the <m>y</m>-value for <m>x_0</m> would be the average of
            <m>y_i</m> and <m>y_j</m>.  This is not a terrible idea, but it is not as effective as
            using the LSRL as described above, since we use the entire dataset when we
            build the coefficients of the LSRL.  So the LSRL will give, by the process of
            interpolation, the best guess for what should be that missing <m>y</m>-value based
            on everything we know, while the <q>average of <m>y_i</m> and <m>y_j</m></q> method only
            pays attention to those two nearest data points and thus may give a very bad
            guess for the corresponding <m>y</m>-value if those two points are not perfectly
            typical, if they have any randomness, any variation in their <m>y</m>-values which
            is not due to the variation of the <m>x</m>.
          </p>
          
          <p>
            It is thus always best to use interpolation as described above.
          </p>
          
          <example xml:id="eg-interpolation">
            <statement>
              <p>
                Working with the statistics students' homework and total course points data
                from Example <xref ref="eg-LSRL"/>, suppose the gradebook of the course instructor
                was somewhat corrupted and the instructor lost the final course points of
                the student Janet.  If Janet's homework points of 77 were not in the corrupted
                part of the gradebook, the instructor might use interpolation to guess what
                Janet's total course point probably were.  To do this, the instructor would
                have plugged in <m>x=77</m> into the equation of the LSRL, <m>\widehat{y}=mx+b</m>
                to get the estimated total course points of <m>.754\cdot77+26.976=85.034</m>.
              </p>
            </statement>
          </example>
          
          <p>
            Another important use of the (coefficients of the) LSRL is to use the
            underlying meanings of the slope and <m>y</m>-intercept.  For this, recall that
            in the equation <m>y=mx+b</m>, the slope <m>m</m> tells us how much the line goes up
            (or down, if the slope is negative) for each increase of the <m>x</m> by one unit,
            while the <m>y</m>-intercept <m>b</m> tells us what would be the <m>y</m> value where the
            line crosses the <m>y</m>-axis, so when the <m>x</m> has the value 0.  In each particular
            situation that we have bivariate quantitative data and compute an LSRL, we
            can then use these interpretations to make statements about the relationship
            between the independent and dependent variables.
          </p>
          
          <example xml:id="eg-LSRLinterpretation">
            <statement>
              <p>
                Look one more time at the data on students' homework and total course points
                in a statistics class from Example <xref ref="eg-LSRL"/>, and the the LSRL computed
                there.  We said that the slope of the LSRL was <m>m=.754</m> and the <m>y</m>-intercept
                was <m>b=26.976</m>.  In context, what this means, is that <em>On average, each
                additional point of homework corresponded to an increase of <m>.754</m> total
                course points.</em>  We may hope that this is actually a causal relationship,
                that the extra work a student does to earn that additional point of homework
                score helps the student learn more statistics and therefore get <m>.75</m> more
                total course points.  But the mathematics here does not require that
                causation, it merely tells us the increase in <m>x</m> is <em>associated</em> with
                that much increase in <m>y</m>.
              </p>
              <p>
                Likewise, we can also conclude from the LSRL that <em>In general, a student
                who did no homework at all would earn about <m>26.976</m> total course points.</em>
                Again, we cannot conclude that doing no homework <em>causes</em> that terrible
                final course point total, only that there is an association.
              </p>
            </statement>
          </example>
          
        </section>
        <section xml:id="sec-Cs">
          <title>Cautions</title>
          
          <subsection xml:id="ssec-S2O">
            <title>Sensitivity to Outliers</title>
            
            <p>
              The correlation coefficient and the (coefficients of the) LSRL are built out
              of means and standard deviations and therefore the following fact is completely
              unsurprising
            </p>
            
            <assemblage xml:id="fact-LSRLsensitive2outliers">
              <title>Fact</title>
              <p>
                The correlation coefficient and the (coefficients of the) LSRL are very
                sensitive to outliers<idx>sensitive to outliers</idx>.
              </p>
            </assemblage>
            
            <p>
              What perhaps is surprising here is that the outliers for bivariate data are
              a little different from those for 1-variable data.
            </p>
            
            <definition xml:id="def-bivariateoutlier">
              <idx><h>outlier</h><h>bivariate</h></idx>
              <statement>
                <p>
                  An <term>outlier</term> for a bivariate quantitative dataset
                  is one which is far away from the curve which has been identified as
                  underlying the shape of the scatterplot of that data.  In particular, a point
                  <m>(x,y)</m> can be a bivariate outlier even if both <m>x</m> is not an outlier for the
                  independent variable data considered alone and <m>y</m> is not an outlier for the
                  dependent variable data alone.
                </p>
              </statement>
            </definition>
            
            <example xml:id="eg-LSRLsensitive">
              <statement>
                <p>
                  Suppose we add one more point <m>(90,30)</m> to the dataset in Example <xref ref="eg-LSRL"/>.
                  Neither the <m>x</m>- nor <m>y</m>-coordinates of this point are outliers with respect to
                  their respective single-coordinate datasets, but it is nevertheless clearly a
                  bivariate outlier, as can be seen in the new scatterplot
                </p>
                <p>
                  [Figure: scatter3.eps - placeholder for scatterplot with outlier]
                </p>
                <p>
                  In fact recomputing the correlation coefficient and LSRL, we find quite a
                  change from what we found before, in Example <xref ref="eg-LSRL"/>:
                </p>
                <me>
                  r=.704\qquad\text{[which used to be $.935$]}
                </me>
                <p>
                  and
                </p>
                <me>
                  \widehat{y}=.529x+38.458\qquad\text{[which used to be $.754x+26.976$]}
                </me>
                <p>
                  all because of one additional point!
                </p>
              </statement>
            </example>
            
          </subsection>
          
          <subsection xml:id="ssec-causation">
            <title>Causation</title>
            
            <p>
              The attentive reader will have noticed that we started our discussion of
              bivariate data by saying we hoped to study when one thing <em>causes</em> another.
              However, what we've actually done instead is find <em>correlation</em> between
              variables, which is quite a different thing.
            </p>
            
            <p>
              Now philosophers have discussed what exactly causation <em>is</em> for
              millennia, so certainly it is a subtle issue that we will not resolve here.
              In fact, careful statisticians usually dodge the complexities by talking about
              <em>relationships</em>, <em>association</em>, and, of course, the
              <em>correlation coefficient</em>, being careful always not to commit to
              <em>causation</em><idx>causation</idx> <mdash/> at least based only on an analysis of the
              statistical data.
            </p>
            
            <p>
              As just one example, where we spoke about the meaning of the square <m>r^2</m> of
              the correlation coefficient (we called it Fact <xref ref="fact-rsquared"/>), we were
              careful to say that <m>r^2</m> measures the variation of the dependent variable
              which is <em>associated</em> with the variation of the independent variable.
              A more reckless description would have been to say that one <em>caused</em> the
              other <mdash/> but don't fall into that trap!
            </p>
            
            <p>
              This would be a bad idea because (among other reasons) the correlation
              coefficient is symmetric in the choice of explanatory and response variables
              (meaning <m>r</m> is the same no matter which is chosen for which role), while any
              reasonable notion of causation is asymmetric.  <em>E.g.,</em> while the
              correlation is exactly the same very large value with either variable being
              <m>x</m> and which <m>y</m>, most people would say that <em>smoking causes cancer</em> and
              not the other way!
            </p>
            
            <p>
              We do need to make one caution about this caution, however.  If there is a
              causal relationship between two variables that are being studied carefully,
              then there will be correlation.  So, to quote the great data scientist
              Edward Tufte<idx>Tufte, Edward</idx>,
            </p>
            
            <blockquote>
              <p>
                <em>Correlation is not causation but it sure is a
                hint.</em><idx><h>correlation is not causation</h><h>but it sure is a hint</h></idx>
              </p>
            </blockquote>
            
            <p>
              The first part of this quote (up to the <q>but</q>) is much more famous and, as
              a very first step, is a good slogan to live by.  Those with a bit more
              statistical sophistication might instead learn this version, though.  A more
              sophisticated-sounding version, again due to Tufte,
              is
            </p>
            
            <blockquote>
              <p>
                <em>Empirically observed covariation is a necessary but not sufficient
                condition for causality.</em>
              </p>
            </blockquote>
            
          </subsection>
          
          <subsection xml:id="ssec-extrapolation">
            <title>Extrapolation</title>
            
            <p>
              We have said that visual intuition often allows humans to sketch fairly good
              approximations of the LSRL on a scatterplot, so long as the correlation
              coefficient tells us there is a strong linear association.  If the diligent
              reader did that with the first scatterplot in Example <xref ref="eg-LSRL"/>, probably
              the resulting line looked much like the line which <term>LibreOffice Calc</term><idx>LibreOffice Calc</idx><idx><h>Calc</h><h>LibreOffice</h></idx> produced
              <mdash/> except humans usually sketch their line all the way to the left and right
              edges of the graphics box.  Automatic tools like <term>LibreOffice Calc</term> do not do
              that, for a reason.
            </p>
            
            <definition xml:id="def-extrapolation">
              <idx>extrapolation</idx>
              <statement>
                <p>
                  Given a bivariate quantitative dataset and associated LSRL with equation
                  <m>\widehat{y}=mx+b</m>, the process of guessing that the value of the dependent
                  variable in this relationship to have the value <m>mx_0+b</m>, for <m>x_0</m> any
                  value for the independent variable which <em>does not satisfy</em>
                  <m>x_{min}\le x_0\le x_{max}</m> [so, instead, either <m>x_0\lt x_{min}</m> or <m>x_0>x_{max}</m>],
                  is called <term>extrapolation</term>.
                </p>
              </statement>
            </definition>
            
            <p>
              Extrapolation is considered a bad, or at least risky, practice.  The idea is
              that we used the evidence in the dataset <m>\{(x_1,y_1), \dots , (x_n,y_n)\}</m> to
              build the LSRL, but, by definition, all of this data lies in the interval on
              the <m>x</m>-axis from <m>x_{min}</m> to <m>x_{max}</m>.  There is literally no evidence from
              this dataset about what the relationship between our chosen explanatory and
              response variables will be for <m>x</m> outside of this interval.  So in the absence
              of strong reasons to believe that the precise linear relationship described
              by the LSRL will continue for more <m>x</m>'s, we should not assume that it does,
              and therefore we should not use the LSRL equation to guess values by
              extrapolation.
            </p>
            
            <p>
              The fact is, however, that often the best thing we can do with available
              information when we want to make predictions out into uncharted territory on
              the <m>x</m>-axis is extrapolation.  So while it is perilous, it is reasonable to
              extrapolate, so long as you are clear about what exactly you are doing.
            </p>
            
            <example xml:id="eg-extrapolation">
              <statement>
                <p>
                  Using again the statistics students' homework and total course points data
                  from Example <xref ref="eg-LSRL"/>, suppose the course instructor wanted to predict
                  what would be the total course points for a student who had earned a perfect
                  <m>100</m> points on their homework.  Plugging into the LSRL, this would have
                  yielded a guess of <m>.754\cdot100+26.976=102.376</m>.  Of course, this would
                  have been impossible, since the maximum possible total course score was <m>100</m>.
                  Moreover, making this guess is an example of extrapolation, since the <m>x</m>
                  value of <m>100</m> is beyond the largest <m>x</m> value of <m>x_{max}=92</m> in the dataset.
                  Therefore we should not rely on this guess <mdash/> as makes sense, since it is
                  invalid by virtue of being larger than <m>100</m>.
                </p>
              </statement>
            </example>
            
          </subsection>
          
          <subsection xml:id="ssec-SP">
            <title>Simpson's Paradox</title>
            
            <p>
              Our last caution is not so much a way using the LSRL can go wrong, but
              instead a warning to be ready for something very counter-intuitive to happen
              <mdash/> so counter-intuitive, in fact, that it is called a paradox.
            </p>
            
            <p>
              It usually seems reasonable that if some object is cut into two pieces, both
              of which have a certain property, then probably the whole object also has
              that same property.  But if the object in question is <em>a population</em> and
              the property is <em>has positive correlation</em>, then maybe the unreasonable
              thing happens.
            </p>
            
            <definition xml:id="def-simpsonsparadox">
              <idx>Simpson's Paradox</idx>
              <statement>
                <p>
                  Suppose we have a population for which we have a bivariate quantitative
                  dataset.  Suppose further that the population is broken into two (or more)
                  subpopulations for all of which the correlation between the two variables
                  is <em>positive</em>, but the correlation of the variables for the whole dataset
                  is <em>negative</em>.  Then this situation is called <term>Simpson's Paradox</term>.
                  [It's also called Simpson's Paradox if the role of <em>positive</em> and
                  <em>negative</em> is reversed in our assumptions.]
                </p>
              </statement>
            </definition>
            
            <p>
              The bad news is that Simpson's paradox can happen.
            </p>
            
            <example xml:id="eg-simpsons1">
              <statement>
                <p>
                  Let <m>\mathbb{P}=\{(0,1), (1,0), (9,10), (10,9)\}</m> be a bivariate dataset, which is
                  broken into the two subpopulations <m>\mathbb{P}_1=\{(0,1), (1,0)\}</m> and
                  <m>\mathbb{P}_2=\{(9,10), (10,9)\}</m>.  Then the correlation coefficients of both <m>\mathbb{P}_1</m>
                  and <m>\mathbb{P}_2</m> are <m>r=-1</m>, but the correlation of all of <m>\mathbb{P}</m> is <m>r=.9756</m>.
                  This is Simpson's Paradox!
                </p>
              </statement>
            </example>
            
            <p>
              Or, in applications, we can have situations like
            </p>
            
            <example xml:id="eg-simpsons2">
              <statement>
                <p>
                  Suppose we collect data on two sections of a statistics course, in particular
                  on how many hours per week the individual students study for the course and
                  how they do in the course, measured by their total course points at the end of
                  the semester.  It is possible that there is a strong positive correlation
                  between these variables for each section by itself, but there is a strong
                  negative correlation when we put all the students into one dataset.  In other
                  words, it is possible that the rational advice, based on both individual
                  sections, is <em>study more and you will do better in the course</em>, but that
                  the rational advice based on all the student data put together is
                  <em>study less and you will do better</em>.
                </p>
              </statement>
            </example>
            
          </subsection>
          
        </section>
        <section xml:id="sec-linear-regression-exercises">
          <title>Exercises</title>
          
          <exercise>
            <statement>
              <p>
                The age (<m>x</m>) and resting heart rate (RHR, <m>y</m>) were measured
                for nine men, yielding this dataset:
              </p>
              <me>
                \begin{matrix}
                x:\amp 20\amp 23\amp 30\amp 37\amp 35\amp 45\amp 51\amp 60\amp 63\\
                y:\amp 72\amp 71\amp 73\amp 74\amp 74\amp 73\amp 75\amp 75\amp 77
                \end{matrix}
              </me>
              <p>
                Make a scatterplot of these data.
              </p>
              <p>
                Based on the scatterplot, what do you think the correlation coefficient <m>r</m> will
                be?
              </p>
              <p>
                Now compute <m>r</m>.
              </p>
              <p>
                Compute the LSRL for these data, write down its equation, and sketch it on top
                of your scatterplot.
              </p>
              <p>
                [You may, of course, do as much of this with electronic tools as you
                like.  However, you should explain what tool you are using, how you used it,
                and what it must have been doing behind the scenes to get the results which it
                displayed and you are turning in.]
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Continuing with the data and computations of the previous
                problem:
              </p>
              <p>
                What percentage of the variation in RHR is associated with variation in age?
              </p>
              <p>
                Write the following sentences with blanks filled in: <q>If I measured the RHR of
                a 55 year-old man, I would expect it to be <m>\underline{\hphantom{XXXX}}</m>.
                Making an estimate like this is called <m>\underline{\hphantom{XXXXXXXXXXX}}</m>.</q>
              </p>
              <p>
                Just looking at the equation of the LSRL, what does it suggest should be the
                RHR of a newborn baby?  Explain.
              </p>
              <p>
                Also explain what an estimate like yours for the RHR of a baby is called.  This
                kind of estimate is considered a bad idea in many cases <mdash/> explain why in
                general, and also use specifics from this particular case.
              </p>
            </statement>
          </exercise>
          
          <exercise>
            <statement>
              <p>
                Write down a bivariate quantitative dataset for a population
                of only two individuals whose LSRL is <m>\widehat{y}=2x-1</m>.
              </p>
              <p>
                What is the correlation coefficient of your dataset?
              </p>
              <p>
                Next, add one more point to the dataset in such a way that you don't change the
                LSRL or correlation coefficient.
              </p>
              <p>
                Finally, can you find a dataset with the same LSRL but having a larger
                correlation coefficient than you just had?
              </p>
              <p>
                [Hint: fool around with modifications or additions to the datasets in
                you already found in this problem, using an electronic tool to do all the
                computational work.  When you find a good one, write it down and explain what
                you thinking was as you searched for it.]
              </p>
            </statement>
          </exercise>
          
        </section>
      </chapter>
    </part>

    <part xml:id="part-GD">
      <title>Good Data</title>

      <chapter xml:id="chap-probability-theory">
        <title>Probability Theory</title>
        <section xml:id="sec-defsforprob">
          <title>Definitions for Probability</title>
          <p/>
          <subsection xml:id="ssec-SSSOaPMs">
            <title>Sample Spaces, Set Operations, and Probability Models</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-VDs">
            <title>Venn Diagrams</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-FPMs">
            <title>Finite Probability Models</title>
            <p/>
          </subsection>
        </section>
        <section xml:id="sec-condprob">
          <title>Conditional Probability</title>
          <p/>
        </section>
        <section xml:id="sec-RVs">
          <title>Random Variables</title>
          <p/>
          <subsection xml:id="ssec-DoRVsaFEs">
            <title>Definition and First Examples</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-D4DRVs">
            <title>Distributions for Discrete RVs</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-expectation4DRVs">
            <title>Expectation for Discrete RVs</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-DF4CRVs">
            <title>Density Functions for Continuous RVs</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-TND">
            <title>The Normal Distribution</title>
            <p/>
          </subsection>
        </section>
        <section xml:id="sec-probability-exercises">
          <title>Exercises</title>
          <p/>
        </section>
      </chapter>

      <chapter xml:id="chap-bringing-home-data">
        <title>Bringing Home the Data</title>
        <section xml:id="sec-SoaSPC">
          <title>Studies of a Population Parameter</title>
          <p/>
        </section>
        <section xml:id="sec-SoC">
          <title>Studies of Causality</title>
          <p/>
          <subsection xml:id="ssec-CGs">
            <title>Control Groups</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-human-subject-experiments-placebo-effect">
            <title>Human-Subject Experiments: The Placebo Effect</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-blinding">
            <title>Blinding</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-CiaRCTs">
            <title>Combining it all: RCTs</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-CLVs">
            <title>Confounded Lurking Variables</title>
            <p/>
          </subsection>
        </section>
        <section xml:id="sec-EE">
          <title>Experimental Ethics</title>
          <p/>
          <subsection xml:id="ssec-DNH">
            <title>"Do No Harm"</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-IC">
            <title>Informed Consent</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-Confidentiality">
            <title>Confidentiality</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-EOIRB">
            <title>External Oversight [IRB]</title>
            <p/>
          </subsection>
        </section>
        <section xml:id="sec-bringing-home-exercises">
          <title>Exercises</title>
          <p/>
        </section>
      </chapter>
    </part>

    <part xml:id="part-IS">
      <title>Inferential Statistics</title>

      <chapter xml:id="chap-IS">
        <title>Basic Inferences</title>
        <section xml:id="sec-CLT">
          <title>The Central Limit Theorem</title>
          <p/>
        </section>
        <section xml:id="sec-BCIs">
          <title>Basic Confidence Intervals</title>
          <p/>
          <subsection xml:id="ssec-CIcautions">
            <title>Cautions</title>
            <p/>
          </subsection>
        </section>
        <section xml:id="sec-BHT">
          <title>Basic Hypothesis Testing</title>
          <p/>
          <subsection xml:id="ssec-tFSoHT">
            <title>The Formal Steps of Hypothesis Testing</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-HSiSEfpvs">
            <title>How Small is Small Enough, for p-values?</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-calculations-hypothesis-testing-population-means">
            <title>Calculations for Hypothesis Testing of Population Means</title>
            <p/>
          </subsection>
          <subsection xml:id="ssec-HTcautions">
            <title>Cautions</title>
            <p/>
          </subsection>
        </section>
        <section xml:id="sec-basic-inferences-exercises">
          <title>Exercises</title>
          <p/>
        </section>
      </chapter>
    </part>
  </book>
</pretext>
